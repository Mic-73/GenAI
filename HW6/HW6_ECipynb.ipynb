{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrneN/3a9R8cYedwrWlumf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mic-73/GenAI/blob/main/HW6/HW6_ECipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Author: Michael Wood\n",
        "\n",
        "Purpose: In this project, I developed a small-scale music generation model using Transformers, specifically designed to generate new musical compositions in the style of Joplin's Piano pieces.\n",
        "\n",
        "Note: Much of the initial code was taken from https://github.com/bforoura/GenAI/tree/main/Module7. The code was modified to fit the requirements of the assignment.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lEmKrQumKNEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "JUahnB1jKQWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preparation"
      ],
      "metadata": {
        "id": "YebNkNBtKXir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Packages\n",
        "\n",
        "!apt-get install musescore\n",
        "!apt-get install music21\n",
        "!apt-get install fluidsynth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmUnWxpOKtqQ",
        "outputId": "ac9e5574-7f8e-4c62-c567-73a8cab9660d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  desktop-file-utils fonts-freefont-ttf libevdev2 libgudev-1.0-0 libinput-bin libinput10 libmd4c0\n",
            "  libmtdev1 libportaudio2 libportmidi0 libqt5core5a libqt5dbus5 libqt5gui5 libqt5help5\n",
            "  libqt5network5 libqt5printsupport5 libqt5qml5 libqt5qmlmodels5 libqt5qmlworkerscript5\n",
            "  libqt5quick5 libqt5sql5 libqt5sql5-sqlite libqt5svg5 libqt5widgets5 libqt5xml5 libqt5xmlpatterns5\n",
            "  libwacom-bin libwacom-common libwacom9 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxkbcommon-x11-0\n",
            "  musescore-common musescore-general-soundfont-small qml-module-qt-labs-folderlistmodel\n",
            "  qml-module-qt-labs-settings qml-module-qtgraphicaleffects qml-module-qtqml\n",
            "  qml-module-qtqml-models2 qml-module-qtquick-controls qml-module-qtquick-dialogs\n",
            "  qml-module-qtquick-layouts qml-module-qtquick-privatewidgets qml-module-qtquick-window2\n",
            "  qml-module-qtquick2 qt5-gtk-platformtheme qttranslations5-l10n\n",
            "Suggested packages:\n",
            "  qt5-image-formats-plugins qtwayland5 qt5-qmltooling-plugins pulseaudio-utils\n",
            "The following NEW packages will be installed:\n",
            "  desktop-file-utils fonts-freefont-ttf libevdev2 libgudev-1.0-0 libinput-bin libinput10 libmd4c0\n",
            "  libmtdev1 libportaudio2 libportmidi0 libqt5core5a libqt5dbus5 libqt5gui5 libqt5help5\n",
            "  libqt5network5 libqt5printsupport5 libqt5qml5 libqt5qmlmodels5 libqt5qmlworkerscript5\n",
            "  libqt5quick5 libqt5sql5 libqt5sql5-sqlite libqt5svg5 libqt5widgets5 libqt5xml5 libqt5xmlpatterns5\n",
            "  libwacom-bin libwacom-common libwacom9 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxkbcommon-x11-0\n",
            "  musescore musescore-common musescore-general-soundfont-small qml-module-qt-labs-folderlistmodel\n",
            "  qml-module-qt-labs-settings qml-module-qtgraphicaleffects qml-module-qtqml\n",
            "  qml-module-qtqml-models2 qml-module-qtquick-controls qml-module-qtquick-dialogs\n",
            "  qml-module-qtquick-layouts qml-module-qtquick-privatewidgets qml-module-qtquick-window2\n",
            "  qml-module-qtquick2 qt5-gtk-platformtheme qttranslations5-l10n\n",
            "0 upgraded, 54 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 63.2 MB of archives.\n",
            "After this operation, 162 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5core5a amd64 5.15.3+dfsg-2ubuntu0.2 [2,006 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevdev2 amd64 1.12.1+dfsg-1 [39.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmtdev1 amd64 1.1.6-1build4 [14.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-common all 2.2.0-1 [54.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom9 amd64 2.2.0-1 [22.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput-bin amd64 1.20.0-1ubuntu0.3 [19.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput10 amd64 1.20.0-1ubuntu0.3 [131 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmd4c0 amd64 0.4.8-1 [42.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5dbus5 amd64 5.15.3+dfsg-2ubuntu0.2 [222 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5network5 amd64 5.15.3+dfsg-2ubuntu0.2 [731 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5gui5 amd64 5.15.3+dfsg-2ubuntu0.2 [3,722 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5widgets5 amd64 5.15.3+dfsg-2ubuntu0.2 [2,561 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5svg5 amd64 5.15.3-1 [149 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5qml5 amd64 5.15.3+dfsg-1 [1,472 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5qmlmodels5 amd64 5.15.3+dfsg-1 [205 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5quick5 amd64 5.15.3+dfsg-1 [1,748 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qml-module-qtquick-window2 amd64 5.15.3+dfsg-1 [26.3 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5qmlworkerscript5 amd64 5.15.3+dfsg-1 [34.4 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qml-module-qtquick2 amd64 5.15.3+dfsg-1 [33.7 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qml-module-qtgraphicaleffects amd64 5.15.3-1 [74.3 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qml-module-qtqml amd64 5.15.3+dfsg-1 [17.2 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qml-module-qtqml-models2 amd64 5.15.3+dfsg-1 [18.0 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qml-module-qtquick-layouts amd64 5.15.3+dfsg-1 [56.0 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qml-module-qtquick-controls amd64 5.15.3-1 [577 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 desktop-file-utils amd64 0.26-1ubuntu3 [55.9 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-freefont-ttf all 20120503-10build1 [2,388 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportmidi0 amd64 1:217-6 [17.8 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5sql5 amd64 5.15.3+dfsg-2ubuntu0.2 [123 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5help5 amd64 5.15.3-1 [162 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5printsupport5 amd64 5.15.3+dfsg-2ubuntu0.2 [214 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5sql5-sqlite amd64 5.15.3+dfsg-2ubuntu0.2 [53.0 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5xml5 amd64 5.15.3+dfsg-2ubuntu0.2 [124 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5xmlpatterns5 amd64 5.15.3-1 [901 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-bin amd64 2.2.0-1 [13.6 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qml-module-qt-labs-folderlistmodel amd64 5.15.3+dfsg-1 [36.1 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qml-module-qt-labs-settings amd64 5.15.3+dfsg-1 [26.5 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qml-module-qtquick-privatewidgets amd64 5.15.3-1 [49.8 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qml-module-qtquick-dialogs amd64 5.15.3-1 [132 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 musescore-general-soundfont-small all 0.2.1-1 [34.1 MB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 musescore-common all 2.3.2+dfsg4-15 [3,332 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 musescore amd64 2.3.2+dfsg4-15 [5,195 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 qt5-gtk-platformtheme amd64 5.15.3+dfsg-2ubuntu0.2 [130 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qttranslations5-l10n all 5.15.3-1 [1,983 kB]\n",
            "Fetched 63.2 MB in 7s (8,822 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libqt5core5a:amd64.\n",
            "(Reading database ... 123630 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libqt5core5a_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libevdev2:amd64.\n",
            "Preparing to unpack .../01-libevdev2_1.12.1+dfsg-1_amd64.deb ...\n",
            "Unpacking libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Selecting previously unselected package libmtdev1:amd64.\n",
            "Preparing to unpack .../02-libmtdev1_1.1.6-1build4_amd64.deb ...\n",
            "Unpacking libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
            "Preparing to unpack .../03-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
            "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Selecting previously unselected package libwacom-common.\n",
            "Preparing to unpack .../04-libwacom-common_2.2.0-1_all.deb ...\n",
            "Unpacking libwacom-common (2.2.0-1) ...\n",
            "Selecting previously unselected package libwacom9:amd64.\n",
            "Preparing to unpack .../05-libwacom9_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom9:amd64 (2.2.0-1) ...\n",
            "Selecting previously unselected package libinput-bin.\n",
            "Preparing to unpack .../06-libinput-bin_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libinput10:amd64.\n",
            "Preparing to unpack .../07-libinput10_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libmd4c0:amd64.\n",
            "Preparing to unpack .../08-libmd4c0_0.4.8-1_amd64.deb ...\n",
            "Unpacking libmd4c0:amd64 (0.4.8-1) ...\n",
            "Selecting previously unselected package libqt5dbus5:amd64.\n",
            "Preparing to unpack .../09-libqt5dbus5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5network5:amd64.\n",
            "Preparing to unpack .../10-libqt5network5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../11-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../12-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../13-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../14-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../15-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../16-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../17-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../18-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../19-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libqt5gui5:amd64.\n",
            "Preparing to unpack .../20-libqt5gui5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5widgets5:amd64.\n",
            "Preparing to unpack .../21-libqt5widgets5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5svg5:amd64.\n",
            "Preparing to unpack .../22-libqt5svg5_5.15.3-1_amd64.deb ...\n",
            "Unpacking libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package libqt5qml5:amd64.\n",
            "Preparing to unpack .../23-libqt5qml5_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking libqt5qml5:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package libqt5qmlmodels5:amd64.\n",
            "Preparing to unpack .../24-libqt5qmlmodels5_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking libqt5qmlmodels5:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package libqt5quick5:amd64.\n",
            "Preparing to unpack .../25-libqt5quick5_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking libqt5quick5:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package qml-module-qtquick-window2:amd64.\n",
            "Preparing to unpack .../26-qml-module-qtquick-window2_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking qml-module-qtquick-window2:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package libqt5qmlworkerscript5:amd64.\n",
            "Preparing to unpack .../27-libqt5qmlworkerscript5_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking libqt5qmlworkerscript5:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package qml-module-qtquick2:amd64.\n",
            "Preparing to unpack .../28-qml-module-qtquick2_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking qml-module-qtquick2:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package qml-module-qtgraphicaleffects:amd64.\n",
            "Preparing to unpack .../29-qml-module-qtgraphicaleffects_5.15.3-1_amd64.deb ...\n",
            "Unpacking qml-module-qtgraphicaleffects:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package qml-module-qtqml:amd64.\n",
            "Preparing to unpack .../30-qml-module-qtqml_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking qml-module-qtqml:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package qml-module-qtqml-models2:amd64.\n",
            "Preparing to unpack .../31-qml-module-qtqml-models2_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking qml-module-qtqml-models2:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package qml-module-qtquick-layouts:amd64.\n",
            "Preparing to unpack .../32-qml-module-qtquick-layouts_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking qml-module-qtquick-layouts:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package qml-module-qtquick-controls:amd64.\n",
            "Preparing to unpack .../33-qml-module-qtquick-controls_5.15.3-1_amd64.deb ...\n",
            "Unpacking qml-module-qtquick-controls:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package desktop-file-utils.\n",
            "Preparing to unpack .../34-desktop-file-utils_0.26-1ubuntu3_amd64.deb ...\n",
            "Unpacking desktop-file-utils (0.26-1ubuntu3) ...\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "Preparing to unpack .../35-fonts-freefont-ttf_20120503-10build1_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-10build1) ...\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "Preparing to unpack .../36-libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportmidi0:amd64.\n",
            "Preparing to unpack .../37-libportmidi0_1%3a217-6_amd64.deb ...\n",
            "Unpacking libportmidi0:amd64 (1:217-6) ...\n",
            "Selecting previously unselected package libqt5sql5:amd64.\n",
            "Preparing to unpack .../38-libqt5sql5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5sql5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5help5:amd64.\n",
            "Preparing to unpack .../39-libqt5help5_5.15.3-1_amd64.deb ...\n",
            "Unpacking libqt5help5:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package libqt5printsupport5:amd64.\n",
            "Preparing to unpack .../40-libqt5printsupport5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5printsupport5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5sql5-sqlite:amd64.\n",
            "Preparing to unpack .../41-libqt5sql5-sqlite_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5sql5-sqlite:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5xml5:amd64.\n",
            "Preparing to unpack .../42-libqt5xml5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5xml5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5xmlpatterns5:amd64.\n",
            "Preparing to unpack .../43-libqt5xmlpatterns5_5.15.3-1_amd64.deb ...\n",
            "Unpacking libqt5xmlpatterns5:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package libwacom-bin.\n",
            "Preparing to unpack .../44-libwacom-bin_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom-bin (2.2.0-1) ...\n",
            "Selecting previously unselected package qml-module-qt-labs-folderlistmodel:amd64.\n",
            "Preparing to unpack .../45-qml-module-qt-labs-folderlistmodel_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking qml-module-qt-labs-folderlistmodel:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package qml-module-qt-labs-settings:amd64.\n",
            "Preparing to unpack .../46-qml-module-qt-labs-settings_5.15.3+dfsg-1_amd64.deb ...\n",
            "Unpacking qml-module-qt-labs-settings:amd64 (5.15.3+dfsg-1) ...\n",
            "Selecting previously unselected package qml-module-qtquick-privatewidgets:amd64.\n",
            "Preparing to unpack .../47-qml-module-qtquick-privatewidgets_5.15.3-1_amd64.deb ...\n",
            "Unpacking qml-module-qtquick-privatewidgets:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package qml-module-qtquick-dialogs:amd64.\n",
            "Preparing to unpack .../48-qml-module-qtquick-dialogs_5.15.3-1_amd64.deb ...\n",
            "Unpacking qml-module-qtquick-dialogs:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package musescore-general-soundfont-small.\n",
            "Preparing to unpack .../49-musescore-general-soundfont-small_0.2.1-1_all.deb ...\n",
            "Unpacking musescore-general-soundfont-small (0.2.1-1) ...\n",
            "Selecting previously unselected package musescore-common.\n",
            "Preparing to unpack .../50-musescore-common_2.3.2+dfsg4-15_all.deb ...\n",
            "Unpacking musescore-common (2.3.2+dfsg4-15) ...\n",
            "Selecting previously unselected package musescore.\n",
            "Preparing to unpack .../51-musescore_2.3.2+dfsg4-15_amd64.deb ...\n",
            "Unpacking musescore (2.3.2+dfsg4-15) ...\n",
            "Selecting previously unselected package qt5-gtk-platformtheme:amd64.\n",
            "Preparing to unpack .../52-qt5-gtk-platformtheme_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package qttranslations5-l10n.\n",
            "Preparing to unpack .../53-qttranslations5-l10n_5.15.3-1_all.deb ...\n",
            "Unpacking qttranslations5-l10n (5.15.3-1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up desktop-file-utils (0.26-1ubuntu3) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up fonts-freefont-ttf (20120503-10build1) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up musescore-general-soundfont-small (0.2.1-1) ...\n",
            "update-alternatives: using /usr/share/sounds/sf3/MuseScore_General_Lite.sf3 to provide /usr/share/sounds/sf3/default-GM.sf3 (default-GM.sf3) in auto mode\n",
            "update-alternatives: using /usr/share/sounds/sf3/MuseScore_General_Lite.sf3 to provide /usr/share/sounds/sf3/MuseScore_General.sf3 (MuseScore_General.sf3) in auto mode\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up musescore-common (2.3.2+dfsg4-15) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up qttranslations5-l10n (5.15.3-1) ...\n",
            "Setting up libportmidi0:amd64 (1:217-6) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Setting up libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libmd4c0:amd64 (0.4.8-1) ...\n",
            "Setting up libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Setting up libwacom-common (2.2.0-1) ...\n",
            "Setting up libwacom9:amd64 (2.2.0-1) ...\n",
            "Setting up libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Setting up libqt5sql5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5xml5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5qml5:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up libwacom-bin (2.2.0-1) ...\n",
            "Setting up libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Setting up libqt5qmlmodels5:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up libqt5sql5-sqlite:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up qml-module-qtqml:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up libqt5xmlpatterns5:amd64 (5.15.3-1) ...\n",
            "Setting up libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5help5:amd64 (5.15.3-1) ...\n",
            "Setting up qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up qml-module-qt-labs-folderlistmodel:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up libqt5qmlworkerscript5:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up qml-module-qt-labs-settings:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up libqt5printsupport5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up qml-module-qtqml-models2:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up libqt5quick5:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Setting up qml-module-qtquick-window2:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up qml-module-qtquick-layouts:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up qml-module-qtquick2:amd64 (5.15.3+dfsg-1) ...\n",
            "Setting up qml-module-qtquick-privatewidgets:amd64 (5.15.3-1) ...\n",
            "Setting up qml-module-qtgraphicaleffects:amd64 (5.15.3-1) ...\n",
            "Setting up qml-module-qtquick-dialogs:amd64 (5.15.3-1) ...\n",
            "Setting up qml-module-qtquick-controls:amd64 (5.15.3-1) ...\n",
            "Setting up musescore (2.3.2+dfsg4-15) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for shared-mime-info (2.1-2) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package music21\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fluid-soundfont-gm libfluidsynth3 libinstpatch-1.0-2 qsynth\n",
            "Suggested packages:\n",
            "  fluid-soundfont-gs jackd\n",
            "The following NEW packages will be installed:\n",
            "  fluid-soundfont-gm fluidsynth libfluidsynth3 libinstpatch-1.0-2 qsynth\n",
            "0 upgraded, 5 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 130 MB of archives.\n",
            "After this operation, 151 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluid-soundfont-gm all 3.1-5.3 [130 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libinstpatch-1.0-2 amd64 1.1.6-1 [240 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfluidsynth3 amd64 2.2.5-1 [246 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluidsynth amd64 2.2.5-1 [27.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qsynth amd64 0.9.6-1 [305 kB]\n",
            "Fetched 130 MB in 10s (13.4 MB/s)\n",
            "Selecting previously unselected package fluid-soundfont-gm.\n",
            "(Reading database ... 125641 files and directories currently installed.)\n",
            "Preparing to unpack .../fluid-soundfont-gm_3.1-5.3_all.deb ...\n",
            "Unpacking fluid-soundfont-gm (3.1-5.3) ...\n",
            "Selecting previously unselected package libinstpatch-1.0-2:amd64.\n",
            "Preparing to unpack .../libinstpatch-1.0-2_1.1.6-1_amd64.deb ...\n",
            "Unpacking libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Selecting previously unselected package libfluidsynth3:amd64.\n",
            "Preparing to unpack .../libfluidsynth3_2.2.5-1_amd64.deb ...\n",
            "Unpacking libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Selecting previously unselected package fluidsynth.\n",
            "Preparing to unpack .../fluidsynth_2.2.5-1_amd64.deb ...\n",
            "Unpacking fluidsynth (2.2.5-1) ...\n",
            "Selecting previously unselected package qsynth.\n",
            "Preparing to unpack .../qsynth_0.9.6-1_amd64.deb ...\n",
            "Unpacking qsynth (0.9.6-1) ...\n",
            "Setting up fluid-soundfont-gm (3.1-5.3) ...\n",
            "update-alternatives: using /usr/share/sounds/sf2/FluidR3_GM.sf2 to provide /usr/share/sounds/sf2/default-GM.sf2 (default-GM.sf2) in auto mode\n",
            "Setting up libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Setting up libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Setting up qsynth (0.9.6-1) ...\n",
            "Setting up fluidsynth (2.2.5-1) ...\n",
            "Created symlink /etc/systemd/user/default.target.wants/fluidsynth.service â†’ /usr/lib/systemd/user/fluidsynth.service.\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for desktop-file-utils (0.26-1ubuntu3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Libraries\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, callbacks\n",
        "\n",
        "import music21\n",
        "import keras\n",
        "\n",
        "from fractions import Fraction\n",
        "\n",
        "#from transformer_utils import (\n",
        " #   parse_midi_files,\n",
        "  #  load_parsed_files,\n",
        "   # get_midi_note,\n",
        "    #SinePositionEncoding,\n",
        "#)"
      ],
      "metadata": {
        "id": "1wrTjfI4KuCf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title transformers_util\n",
        "\n",
        "def parse_midi_files(file_list, parser, seq_len, parsed_data_path=None):\n",
        "    notes_list = []\n",
        "    duration_list = []\n",
        "    notes = []\n",
        "    durations = []\n",
        "\n",
        "    for i, file in enumerate(file_list):\n",
        "        print(i + 1, \"Parsing %s\" % file)\n",
        "        score = parser.parse(file).chordify()\n",
        "\n",
        "        notes.append(\"START\")\n",
        "        durations.append(\"0.0\")\n",
        "\n",
        "        for element in score.flat:\n",
        "            note_name = None\n",
        "            duration_name = None\n",
        "\n",
        "            if isinstance(element, music21.key.Key):\n",
        "                note_name = str(element.tonic.name) + \":\" + str(element.mode)\n",
        "                duration_name = \"0.0\"\n",
        "\n",
        "            elif isinstance(element, music21.meter.TimeSignature):\n",
        "                note_name = str(element.ratioString) + \"TS\"\n",
        "                duration_name = \"0.0\"\n",
        "\n",
        "            elif isinstance(element, music21.chord.Chord):\n",
        "                note_name = element.pitches[-1].nameWithOctave\n",
        "                duration_name = str(element.duration.quarterLength)\n",
        "\n",
        "            elif isinstance(element, music21.note.Rest):\n",
        "                note_name = str(element.name)\n",
        "                duration_name = str(element.duration.quarterLength)\n",
        "\n",
        "            elif isinstance(element, music21.note.Note):\n",
        "                note_name = str(element.nameWithOctave)\n",
        "                duration_name = str(element.duration.quarterLength)\n",
        "\n",
        "            if note_name and duration_name:\n",
        "                notes.append(note_name)\n",
        "                durations.append(duration_name)\n",
        "        print(f\"{len(notes)} notes parsed\")\n",
        "\n",
        "    notes_list = []\n",
        "    duration_list = []\n",
        "\n",
        "    print(f\"Building sequences of length {seq_len}\")\n",
        "    for i in range(len(notes) - seq_len):\n",
        "        notes_list.append(\" \".join(notes[i : (i + seq_len)]))\n",
        "        duration_list.append(\" \".join(durations[i : (i + seq_len)]))\n",
        "\n",
        "    if parsed_data_path:\n",
        "        with open(os.path.join(parsed_data_path, \"notes\"), \"wb\") as f:\n",
        "            pkl.dump(notes_list, f)\n",
        "        with open(os.path.join(parsed_data_path, \"durations\"), \"wb\") as f:\n",
        "            pkl.dump(duration_list, f)\n",
        "\n",
        "    return notes_list, duration_list\n",
        "\n",
        "\n",
        "def load_parsed_files(parsed_data_path):\n",
        "    with open(os.path.join(parsed_data_path, \"notes\"), \"rb\") as f:\n",
        "        notes = pkl.load(f)\n",
        "    with open(os.path.join(parsed_data_path, \"durations\"), \"rb\") as f:\n",
        "        durations = pkl.load(f)\n",
        "    return notes, durations\n",
        "\n",
        "\n",
        "def get_midi_note(sample_note, sample_duration):\n",
        "    new_note = None\n",
        "\n",
        "    if \"TS\" in sample_note:\n",
        "        new_note = music21.meter.TimeSignature(sample_note.split(\"TS\")[0])\n",
        "\n",
        "    elif \"major\" in sample_note or \"minor\" in sample_note:\n",
        "        tonic, mode = sample_note.split(\":\")\n",
        "        new_note = music21.key.Key(tonic, mode)\n",
        "\n",
        "    elif sample_note == \"rest\":\n",
        "        new_note = music21.note.Rest()\n",
        "        new_note.duration = music21.duration.Duration(\n",
        "            float(Fraction(sample_duration))\n",
        "        )\n",
        "        new_note.storedInstrument = music21.instrument.Violoncello()\n",
        "\n",
        "    elif \".\" in sample_note:\n",
        "        notes_in_chord = sample_note.split(\".\")\n",
        "        chord_notes = []\n",
        "        for current_note in notes_in_chord:\n",
        "            n = music21.note.Note(current_note)\n",
        "            n.duration = music21.duration.Duration(\n",
        "                float(Fraction(sample_duration))\n",
        "            )\n",
        "            n.storedInstrument = music21.instrument.Violoncello()\n",
        "            chord_notes.append(n)\n",
        "        new_note = music21.chord.Chord(chord_notes)\n",
        "\n",
        "    elif sample_note == \"rest\":\n",
        "        new_note = music21.note.Rest()\n",
        "        new_note.duration = music21.duration.Duration(\n",
        "            float(Fraction(sample_duration))\n",
        "        )\n",
        "        new_note.storedInstrument = music21.instrument.Violoncello()\n",
        "\n",
        "    elif sample_note != \"START\":\n",
        "        new_note = music21.note.Note(sample_note)\n",
        "        new_note.duration = music21.duration.Duration(\n",
        "            float(Fraction(sample_duration))\n",
        "        )\n",
        "        new_note.storedInstrument = music21.instrument.Violoncello()\n",
        "\n",
        "    return new_note\n",
        "\n",
        "\n",
        "class SinePositionEncoding(keras.layers.Layer):\n",
        "    \"\"\"Sinusoidal positional encoding layer.\n",
        "    This layer calculates the position encoding as a mix of sine and cosine\n",
        "    functions with geometrically increasing wavelengths. Defined and formulized\n",
        "    in [Attention is All You Need](https://arxiv.org/abs/1706.03762).\n",
        "    Takes as input an embedded token tensor. The input must have shape\n",
        "    [batch_size, sequence_length, feature_size]. This layer will return a\n",
        "    positional encoding the same size as the embedded token tensor, which\n",
        "    can be added directly to the embedded token tensor.\n",
        "    Args:\n",
        "        max_wavelength: The maximum angular wavelength of the sine/cosine\n",
        "            curves, as described in Attention is All You Need. Defaults to\n",
        "            10000.\n",
        "    Examples:\n",
        "    ```python\n",
        "    # create a simple embedding layer with sinusoidal positional encoding\n",
        "    seq_len = 100\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 32\n",
        "    inputs = keras.Input((seq_len,), dtype=tf.float32)\n",
        "    embedding = keras.layers.Embedding(\n",
        "        input_dim=vocab_size, output_dim=embedding_dim\n",
        "    )(inputs)\n",
        "    positional_encoding = keras_nlp.layers.SinePositionEncoding()(embedding)\n",
        "    outputs = embedding + positional_encoding\n",
        "    ```\n",
        "    References:\n",
        "     - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_wavelength=10000,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_wavelength = max_wavelength\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # TODO(jbischof): replace `hidden_size` with`hidden_dim` for consistency\n",
        "        # with other layers.\n",
        "        input_shape = tf.shape(inputs)\n",
        "        # length of sequence is the second last dimension of the inputs\n",
        "        seq_length = input_shape[-2]\n",
        "        hidden_size = input_shape[-1]\n",
        "        position = tf.cast(tf.range(seq_length), self.compute_dtype)\n",
        "        min_freq = tf.cast(1 / self.max_wavelength, dtype=self.compute_dtype)\n",
        "        timescales = tf.pow(\n",
        "            min_freq,\n",
        "            tf.cast(2 * (tf.range(hidden_size) // 2), self.compute_dtype)\n",
        "            / tf.cast(hidden_size, self.compute_dtype),\n",
        "        )\n",
        "        angles = tf.expand_dims(position, 1) * tf.expand_dims(timescales, 0)\n",
        "        # even indices are sine, odd are cosine\n",
        "        cos_mask = tf.cast(tf.range(hidden_size) % 2, self.compute_dtype)\n",
        "        sin_mask = 1 - cos_mask\n",
        "        # embedding shape is [seq_length, hidden_size]\n",
        "        positional_encodings = (\n",
        "            tf.sin(angles) * sin_mask + tf.cos(angles) * cos_mask\n",
        "        )\n",
        "\n",
        "        return tf.broadcast_to(positional_encodings, input_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"max_wavelength\": self.max_wavelength,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ],
      "metadata": {
        "id": "1bSKQMuX8V6V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PARAMETERS\n",
        "\n",
        "PARSE_MIDI_FILES = True\n",
        "PARSED_DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/\"\n",
        "DATASET_REPETITIONS = 1\n",
        "\n",
        "SEQ_LEN = 50\n",
        "EMBEDDING_DIM = 256\n",
        "KEY_DIM = 256\n",
        "N_HEADS = 5\n",
        "DROPOUT_RATE = 0.3\n",
        "FEED_FORWARD_DIM = 256\n",
        "LOAD_MODEL = False\n",
        "\n",
        "# optimization\n",
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "GENERATE_LEN = 50"
      ],
      "metadata": {
        "id": "FP2M_z3UKuEl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVw87BOEW1ct",
        "outputId": "eb17e222-96c2-4bb8-9f4c-8d034f8ca1a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the Data\n",
        "\n",
        "file_list = glob.glob(\"/content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/*.mid\")\n",
        "print(f\"Found {len(file_list)} midi files\")\n",
        "parser = music21.converter\n",
        "\n",
        "if PARSE_MIDI_FILES:\n",
        "    notes, durations = parse_midi_files(\n",
        "        file_list, parser, SEQ_LEN + 1, PARSED_DATA_PATH\n",
        "    )\n",
        "else:\n",
        "    notes, durations = load_parsed_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCo0uhD0LKj6",
        "outputId": "ef6da804-41cb-485d-a945-c384d40c4e0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 42 midi files\n",
            "1 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/jopnew.mid\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-df681f766603>:8: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
            "  notes, durations = parse_midi_files(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1047 notes parsed\n",
            "2 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjwinner.mid\n",
            "2131 notes parsed\n",
            "3 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjalabam.mid\n",
            "3243 notes parsed\n",
            "4 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjpeach.mid\n",
            "4170 notes parsed\n",
            "5 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjfavour.mid\n",
            "5111 notes parsed\n",
            "6 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjpin.mid\n",
            "6119 notes parsed\n",
            "7 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjeuph.mid\n",
            "7123 notes parsed\n",
            "8 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjpalm.mid\n",
            "8068 notes parsed\n",
            "9 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjnon.mid\n",
            "8975 notes parsed\n",
            "10 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjdoing.mid\n",
            "10044 notes parsed\n",
            "11 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjtsl.mid\n",
            "11064 notes parsed\n",
            "12 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjslr.mid\n",
            "12072 notes parsed\n",
            "13 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjcascad.mid\n",
            "13096 notes parsed\n",
            "14 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjcom.mid\n",
            "13818 notes parsed\n",
            "15 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjeugen.mid\n",
            "15027 notes parsed\n",
            "16 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjhfr.mid\n",
            "15935 notes parsed\n",
            "17 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/joprag.mid\n",
            "16909 notes parsed\n",
            "18 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjhkr.mid\n",
            "17885 notes parsed\n",
            "19 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjmaj.mid\n",
            "18657 notes parsed\n",
            "20 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjswipes.mid\n",
            "19744 notes parsed\n",
            "21 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjrbm.mid\n",
            "20538 notes parsed\n",
            "22 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjcru.mid\n",
            "21589 notes parsed\n",
            "23 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjpar.mid\n",
            "22608 notes parsed\n",
            "24 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/binks_wa.mid\n",
            "23454 notes parsed\n",
            "25 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjsycam.mid\n",
            "24328 notes parsed\n",
            "26 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjscn.mid\n",
            "25274 notes parsed\n",
            "27 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjros.mid\n",
            "26344 notes parsed\n",
            "28 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/jopfiglf.mid\n",
            "27624 notes parsed\n",
            "29 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjleo.mid\n",
            "28654 notes parsed\n",
            "30 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/joplin-1.mid\n",
            "29713 notes parsed\n",
            "31 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjchrysa.mid\n",
            "30807 notes parsed\n",
            "32 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/joplin_.mid\n",
            "31864 notes parsed\n",
            "33 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjant.mid\n",
            "32683 notes parsed\n",
            "34 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/joplin.mid\n",
            "33721 notes parsed\n",
            "35 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/jopmag.mid\n",
            "35007 notes parsed\n",
            "36 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/hm_sjstp.mid\n",
            "35836 notes parsed\n",
            "37 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/joplin_s.mid\n",
            "36832 notes parsed\n",
            "38 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/joplin_m.mid\n",
            "37862 notes parsed\n",
            "39 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/jopwalls.mid\n",
            "38886 notes parsed\n",
            "40 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/sjwillow.mid\n",
            "39931 notes parsed\n",
            "41 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/cleopha.mid\n",
            "41121 notes parsed\n",
            "42 Parsing /content/drive/My Drive/Colab Notebooks/GenAI/joplin_mid_files/gladiols.mid\n",
            "42870 notes parsed\n",
            "Building sequences of length 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenization"
      ],
      "metadata": {
        "id": "c270VSNJKfk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create the Dataset\n",
        "\n",
        "def create_dataset(elements):\n",
        "    ds = (\n",
        "        tf.data.Dataset.from_tensor_slices(elements)\n",
        "        .batch(BATCH_SIZE, drop_remainder=True)\n",
        "        .shuffle(1000)\n",
        "    )\n",
        "    vectorize_layer = layers.TextVectorization(\n",
        "        standardize=None, output_mode=\"int\"\n",
        "    )\n",
        "    vectorize_layer.adapt(ds)\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    return ds, vectorize_layer, vocab\n",
        "\n",
        "\n",
        "notes_seq_ds, notes_vectorize_layer, notes_vocab = create_dataset(notes)\n",
        "durations_seq_ds, durations_vectorize_layer, durations_vocab = create_dataset(\n",
        "    durations\n",
        ")\n",
        "seq_ds = tf.data.Dataset.zip((notes_seq_ds, durations_seq_ds))\n",
        "\n",
        "notes_vocab_size = len(notes_vocab)\n",
        "durations_vocab_size = len(durations_vocab)"
      ],
      "metadata": {
        "id": "L9sDWmNgKujW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create the Training Set\n",
        "\n",
        "# Create the training set of sequences and the same sequences shifted by one note\n",
        "def prepare_inputs(notes, durations):\n",
        "    notes = tf.expand_dims(notes, -1)\n",
        "    durations = tf.expand_dims(durations, -1)\n",
        "    tokenized_notes = notes_vectorize_layer(notes)\n",
        "    tokenized_durations = durations_vectorize_layer(durations)\n",
        "    x = (tokenized_notes[:, :-1], tokenized_durations[:, :-1])\n",
        "    y = (tokenized_notes[:, 1:], tokenized_durations[:, 1:])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "ds = seq_ds.map(prepare_inputs).repeat(DATASET_REPETITIONS)"
      ],
      "metadata": {
        "id": "63EkQWQVKumN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title causal_attention_mask Function\n",
        "\n",
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a9oVLZAKuoS",
        "outputId": "4794bc3c-9d21-4524-b6d1-8a0fa55cbef5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TransformerBlock Class (Create Transformer Block Layer)\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads,\n",
        "        key_dim,\n",
        "        embed_dim,\n",
        "        ff_dim,\n",
        "        name,\n",
        "        dropout_rate=DROPOUT_RATE,\n",
        "    ):\n",
        "        super(TransformerBlock, self).__init__(name=name)\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attn = layers.MultiHeadAttention(\n",
        "            num_heads, key_dim, output_shape=embed_dim\n",
        "        )\n",
        "        self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
        "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "        self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(\n",
        "            batch_size, seq_len, seq_len, tf.bool\n",
        "        )\n",
        "        attention_output, attention_scores = self.attn(\n",
        "            inputs,\n",
        "            inputs,\n",
        "            attention_mask=causal_mask,\n",
        "            return_attention_scores=True,\n",
        "        )\n",
        "        attention_output = self.dropout_1(attention_output)\n",
        "        out1 = self.ln_1(inputs + attention_output)\n",
        "        ffn_1 = self.ffn_1(out1)\n",
        "        ffn_2 = self.ffn_2(ffn_1)\n",
        "        ffn_output = self.dropout_2(ffn_2)\n",
        "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"key_dim\": self.key_dim,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"ff_dim\": self.ff_dim,\n",
        "                \"dropout_rate\": self.dropout_rate,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ],
      "metadata": {
        "id": "6L2TlUbWMEB-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TokenAndPositionEmbedding Class (Create the Token and Position Embedding)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.token_emb = layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embed_dim,\n",
        "            embeddings_initializer=\"he_uniform\",\n",
        "        )\n",
        "        self.pos_emb = SinePositionEncoding()\n",
        "\n",
        "    def call(self, x):\n",
        "        embedding = self.token_emb(x)\n",
        "        positions = self.pos_emb(embedding)\n",
        "        return embedding + positions\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ],
      "metadata": {
        "id": "Y8WH5ID4MOUl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Architecture"
      ],
      "metadata": {
        "id": "--0Yk-5CKfr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create the Model\n",
        "\n",
        "note_inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "durations_inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "note_embeddings = TokenAndPositionEmbedding(\n",
        "    notes_vocab_size, EMBEDDING_DIM // 2\n",
        ")(note_inputs)\n",
        "duration_embeddings = TokenAndPositionEmbedding(\n",
        "    durations_vocab_size, EMBEDDING_DIM // 2\n",
        ")(durations_inputs)\n",
        "embeddings = layers.Concatenate()([note_embeddings, duration_embeddings])\n",
        "x, attention_scores = TransformerBlock(\n",
        "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM, name=\"attention\"\n",
        ")(embeddings)\n",
        "note_outputs = layers.Dense(\n",
        "    notes_vocab_size, activation=\"softmax\", name=\"note_outputs\"\n",
        ")(x)\n",
        "duration_outputs = layers.Dense(\n",
        "    durations_vocab_size, activation=\"softmax\", name=\"duration_outputs\"\n",
        ")(x)\n",
        "model = models.Model(\n",
        "    inputs=[note_inputs, durations_inputs],\n",
        "    outputs=[note_outputs, duration_outputs],  # attention_scores\n",
        ")\n",
        "model.compile(\n",
        "    \"adam\",\n",
        "    loss=[\n",
        "        losses.SparseCategoricalCrossentropy(),\n",
        "        losses.SparseCategoricalCrossentropy(),\n",
        "    ],\n",
        ")\n",
        "att_model = models.Model(\n",
        "    inputs=[note_inputs, durations_inputs], outputs=attention_scores\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "Hia9sNTKKvJL",
        "outputId": "aaa1a7b6-328f-4755-b6cf-dc38212eb0ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           â”‚              \u001b[38;5;34m0\u001b[0m â”‚ -                      â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ input_layer_1             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           â”‚              \u001b[38;5;34m0\u001b[0m â”‚ -                      â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ token_and_position_embedâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚         \u001b[38;5;34m10,496\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
              "â”‚ (\u001b[38;5;33mTokenAndPositionEmbeddiâ€¦\u001b[0m â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ token_and_position_embedâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚         \u001b[38;5;34m10,496\u001b[0m â”‚ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
              "â”‚ (\u001b[38;5;33mTokenAndPositionEmbeddiâ€¦\u001b[0m â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â”‚              \u001b[38;5;34m0\u001b[0m â”‚ token_and_position_emâ€¦ â”‚\n",
              "â”‚                           â”‚                        â”‚                â”‚ token_and_position_emâ€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ attention                 â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),    â”‚      \u001b[38;5;34m1,447,424\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
              "â”‚ (\u001b[38;5;33mTransformerBlock\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)] â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ note_outputs (\u001b[38;5;33mDense\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m)       â”‚         \u001b[38;5;34m21,074\u001b[0m â”‚ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ duration_outputs (\u001b[38;5;33mDense\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m)       â”‚         \u001b[38;5;34m21,074\u001b[0m â”‚ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)              </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">        Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to           </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                      â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ input_layer_1             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                      â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ token_and_position_embedâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">10,496</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbeddiâ€¦</span> â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ token_and_position_embedâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">10,496</span> â”‚ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbeddiâ€¦</span> â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ token_and_position_emâ€¦ â”‚\n",
              "â”‚                           â”‚                        â”‚                â”‚ token_and_position_emâ€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ attention                 â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,447,424</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)] â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ note_outputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>)       â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">21,074</span> â”‚ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ duration_outputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>)       â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">21,074</span> â”‚ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,510,564\u001b[0m (5.76 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,510,564</span> (5.76 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,510,564\u001b[0m (5.76 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,510,564</span> (5.76 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training"
      ],
      "metadata": {
        "id": "jwHQr9-mKfyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MusicGenerator Class (Create a MusicGenerator Checkpoint)\n",
        "\n",
        "class MusicGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_note, index_to_duration, top_k=10):\n",
        "        self.index_to_note = index_to_note\n",
        "        self.note_to_index = {\n",
        "            note: index for index, note in enumerate(index_to_note)\n",
        "        }\n",
        "        self.index_to_duration = index_to_duration\n",
        "        self.duration_to_index = {\n",
        "            duration: index for index, duration in enumerate(index_to_duration)\n",
        "        }\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def get_note(self, notes, durations, temperature):\n",
        "        sample_note_idx = 1\n",
        "        while sample_note_idx == 1:\n",
        "            sample_note_idx, note_probs = self.sample_from(\n",
        "                notes[0][-1], temperature\n",
        "            )\n",
        "            sample_note = self.index_to_note[sample_note_idx]\n",
        "\n",
        "        sample_duration_idx = 1\n",
        "        while sample_duration_idx == 1:\n",
        "            sample_duration_idx, duration_probs = self.sample_from(\n",
        "                durations[0][-1], temperature\n",
        "            )\n",
        "            sample_duration = self.index_to_duration[sample_duration_idx]\n",
        "\n",
        "        new_note = get_midi_note(sample_note, sample_duration)\n",
        "\n",
        "        return (\n",
        "            new_note,\n",
        "            sample_note_idx,\n",
        "            sample_note,\n",
        "            note_probs,\n",
        "            sample_duration_idx,\n",
        "            sample_duration,\n",
        "            duration_probs,\n",
        "        )\n",
        "\n",
        "    def generate(self, start_notes, start_durations, max_tokens, temperature):\n",
        "        attention_model = models.Model(\n",
        "            inputs=self.model.input,\n",
        "            outputs=self.model.get_layer(\"attention\").output,\n",
        "        )\n",
        "\n",
        "        start_note_tokens = [self.note_to_index.get(x, 1) for x in start_notes]\n",
        "        start_duration_tokens = [\n",
        "            self.duration_to_index.get(x, 1) for x in start_durations\n",
        "        ]\n",
        "        sample_note = None\n",
        "        sample_duration = None\n",
        "        info = []\n",
        "        midi_stream = music21.stream.Stream()\n",
        "\n",
        "        midi_stream.append(music21.clef.BassClef())\n",
        "\n",
        "        for sample_note, sample_duration in zip(start_notes, start_durations):\n",
        "            new_note = get_midi_note(sample_note, sample_duration)\n",
        "            if new_note is not None:\n",
        "                midi_stream.append(new_note)\n",
        "\n",
        "        while len(start_note_tokens) < max_tokens:\n",
        "            x1 = np.array([start_note_tokens])\n",
        "            x2 = np.array([start_duration_tokens])\n",
        "            notes, durations = self.model.predict([x1, x2], verbose=0)\n",
        "\n",
        "            repeat = True\n",
        "\n",
        "            while repeat:\n",
        "                (\n",
        "                    new_note,\n",
        "                    sample_note_idx,\n",
        "                    sample_note,\n",
        "                    note_probs,\n",
        "                    sample_duration_idx,\n",
        "                    sample_duration,\n",
        "                    duration_probs,\n",
        "                ) = self.get_note(notes, durations, temperature)\n",
        "\n",
        "                if (\n",
        "                    isinstance(new_note, music21.chord.Chord)\n",
        "                    or isinstance(new_note, music21.note.Note)\n",
        "                    or isinstance(new_note, music21.note.Rest)\n",
        "                ) and sample_duration == \"0.0\":\n",
        "                    repeat = True\n",
        "                else:\n",
        "                    repeat = False\n",
        "\n",
        "            if new_note is not None:\n",
        "                midi_stream.append(new_note)\n",
        "\n",
        "            _, att = attention_model.predict([x1, x2], verbose=0)\n",
        "\n",
        "            info.append(\n",
        "                {\n",
        "                    \"prompt\": [start_notes.copy(), start_durations.copy()],\n",
        "                    \"midi\": midi_stream,\n",
        "                    \"chosen_note\": (sample_note, sample_duration),\n",
        "                    \"note_probs\": note_probs,\n",
        "                    \"duration_probs\": duration_probs,\n",
        "                    \"atts\": att[0, :, -1, :],\n",
        "                }\n",
        "            )\n",
        "            start_note_tokens.append(sample_note_idx)\n",
        "            start_duration_tokens.append(sample_duration_idx)\n",
        "            start_notes.append(sample_note)\n",
        "            start_durations.append(sample_duration)\n",
        "\n",
        "            if sample_note == \"START\":\n",
        "                break\n",
        "\n",
        "        return info\n",
        "\n",
        "    '''\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        info = self.generate(\n",
        "            [\"START\"], [\"0.0\"], max_tokens=GENERATE_LEN, temperature=0.5\n",
        "        )\n",
        "        midi_stream = info[-1][\"midi\"].chordify()\n",
        "        print(info[-1][\"prompt\"])\n",
        "        #midi_stream.show()\n",
        "        midi_stream.write(\n",
        "            \"midi\",\n",
        "            fp=os.path.join(\n",
        "                \"/content/drive/My Drive/Colab Notebooks/GenAI/output_joplin/\",\n",
        "                \"output-\" + str(epoch).zfill(4) + \".mid\",\n",
        "            ),\n",
        "        )\n",
        "        '''"
      ],
      "metadata": {
        "id": "DrEOf-SXKvo8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train the Model\n",
        "\n",
        "# Tokenize starting prompt\n",
        "music_generator = MusicGenerator(notes_vocab, durations_vocab)\n",
        "EPOCHS=500\n",
        "model.fit(\n",
        "    ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[\n",
        "        music_generator,\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7HfZDrMKvrj",
        "outputId": "bc9f4ce2-7409-4862-c30f-8d790f62701c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7147 - loss: 3.1058 - note_outputs_loss: 2.3911\n",
            "Epoch 2/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7164 - loss: 3.0435 - note_outputs_loss: 2.3271\n",
            "Epoch 3/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6864 - loss: 2.9903 - note_outputs_loss: 2.3039\n",
            "Epoch 4/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7188 - loss: 2.9893 - note_outputs_loss: 2.2704\n",
            "Epoch 5/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.6914 - loss: 2.9328 - note_outputs_loss: 2.2414\n",
            "Epoch 6/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - duration_outputs_loss: 0.7342 - loss: 2.8997 - note_outputs_loss: 2.1655\n",
            "Epoch 7/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - duration_outputs_loss: 0.7345 - loss: 2.8930 - note_outputs_loss: 2.1586\n",
            "Epoch 8/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7078 - loss: 2.8079 - note_outputs_loss: 2.1001\n",
            "Epoch 9/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7156 - loss: 2.7870 - note_outputs_loss: 2.0714\n",
            "Epoch 10/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7162 - loss: 2.7557 - note_outputs_loss: 2.0395\n",
            "Epoch 11/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7178 - loss: 2.7356 - note_outputs_loss: 2.0178\n",
            "Epoch 12/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7177 - loss: 2.6916 - note_outputs_loss: 1.9739\n",
            "Epoch 13/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.6947 - loss: 2.6400 - note_outputs_loss: 1.9453\n",
            "Epoch 14/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - duration_outputs_loss: 0.7137 - loss: 2.6412 - note_outputs_loss: 1.9275\n",
            "Epoch 15/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6928 - loss: 2.6086 - note_outputs_loss: 1.9158\n",
            "Epoch 16/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6787 - loss: 2.5470 - note_outputs_loss: 1.8683\n",
            "Epoch 17/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7175 - loss: 2.5796 - note_outputs_loss: 1.8620\n",
            "Epoch 18/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - duration_outputs_loss: 0.7246 - loss: 2.5460 - note_outputs_loss: 1.8214\n",
            "Epoch 19/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7089 - loss: 2.5257 - note_outputs_loss: 1.8168\n",
            "Epoch 20/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7204 - loss: 2.5311 - note_outputs_loss: 1.8107\n",
            "Epoch 21/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - duration_outputs_loss: 0.7343 - loss: 2.5138 - note_outputs_loss: 1.7794\n",
            "Epoch 22/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7271 - loss: 2.4942 - note_outputs_loss: 1.7671\n",
            "Epoch 23/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6948 - loss: 2.4140 - note_outputs_loss: 1.7192\n",
            "Epoch 24/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6899 - loss: 2.4065 - note_outputs_loss: 1.7166\n",
            "Epoch 25/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7226 - loss: 2.4293 - note_outputs_loss: 1.7067\n",
            "Epoch 26/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7130 - loss: 2.4173 - note_outputs_loss: 1.7043\n",
            "Epoch 27/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7307 - loss: 2.4144 - note_outputs_loss: 1.6837\n",
            "Epoch 28/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7299 - loss: 2.3899 - note_outputs_loss: 1.6599\n",
            "Epoch 29/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7011 - loss: 2.3734 - note_outputs_loss: 1.6723\n",
            "Epoch 30/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7074 - loss: 2.3689 - note_outputs_loss: 1.6615\n",
            "Epoch 31/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7029 - loss: 2.3604 - note_outputs_loss: 1.6576\n",
            "Epoch 32/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7159 - loss: 2.3332 - note_outputs_loss: 1.6172\n",
            "Epoch 33/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7057 - loss: 2.3145 - note_outputs_loss: 1.6088\n",
            "Epoch 34/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7421 - loss: 2.3385 - note_outputs_loss: 1.5964\n",
            "Epoch 35/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7254 - loss: 2.3362 - note_outputs_loss: 1.6107\n",
            "Epoch 36/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6989 - loss: 2.2991 - note_outputs_loss: 1.6002\n",
            "Epoch 37/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7175 - loss: 2.2930 - note_outputs_loss: 1.5755\n",
            "Epoch 38/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7203 - loss: 2.2836 - note_outputs_loss: 1.5633\n",
            "Epoch 39/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7075 - loss: 2.2722 - note_outputs_loss: 1.5646\n",
            "Epoch 40/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7335 - loss: 2.2770 - note_outputs_loss: 1.5435\n",
            "Epoch 41/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6881 - loss: 2.2321 - note_outputs_loss: 1.5440\n",
            "Epoch 42/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7308 - loss: 2.2614 - note_outputs_loss: 1.5307\n",
            "Epoch 43/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7122 - loss: 2.2410 - note_outputs_loss: 1.5288\n",
            "Epoch 44/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7199 - loss: 2.2442 - note_outputs_loss: 1.5244\n",
            "Epoch 45/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7092 - loss: 2.2439 - note_outputs_loss: 1.5347\n",
            "Epoch 46/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6950 - loss: 2.2063 - note_outputs_loss: 1.5113\n",
            "Epoch 47/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7371 - loss: 2.2529 - note_outputs_loss: 1.5158\n",
            "Epoch 48/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6995 - loss: 2.1845 - note_outputs_loss: 1.4850\n",
            "Epoch 49/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7287 - loss: 2.2292 - note_outputs_loss: 1.5005\n",
            "Epoch 50/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7312 - loss: 2.2253 - note_outputs_loss: 1.4941\n",
            "Epoch 51/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6996 - loss: 2.1638 - note_outputs_loss: 1.4641\n",
            "Epoch 52/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7202 - loss: 2.1935 - note_outputs_loss: 1.4733\n",
            "Epoch 53/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7215 - loss: 2.1766 - note_outputs_loss: 1.4551\n",
            "Epoch 54/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7321 - loss: 2.1812 - note_outputs_loss: 1.4491\n",
            "Epoch 55/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7472 - loss: 2.2102 - note_outputs_loss: 1.4630\n",
            "Epoch 56/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7307 - loss: 2.1801 - note_outputs_loss: 1.4494\n",
            "Epoch 57/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7220 - loss: 2.1877 - note_outputs_loss: 1.4657\n",
            "Epoch 58/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7282 - loss: 2.1920 - note_outputs_loss: 1.4637\n",
            "Epoch 59/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7327 - loss: 2.1687 - note_outputs_loss: 1.4360\n",
            "Epoch 60/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7340 - loss: 2.1867 - note_outputs_loss: 1.4527\n",
            "Epoch 61/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7075 - loss: 2.1569 - note_outputs_loss: 1.4494\n",
            "Epoch 62/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7363 - loss: 2.1678 - note_outputs_loss: 1.4315\n",
            "Epoch 63/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7111 - loss: 2.1483 - note_outputs_loss: 1.4372\n",
            "Epoch 64/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7446 - loss: 2.1584 - note_outputs_loss: 1.4138\n",
            "Epoch 65/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7243 - loss: 2.1298 - note_outputs_loss: 1.4055\n",
            "Epoch 66/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7080 - loss: 2.1238 - note_outputs_loss: 1.4159\n",
            "Epoch 67/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7357 - loss: 2.1348 - note_outputs_loss: 1.3991\n",
            "Epoch 68/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6859 - loss: 2.0885 - note_outputs_loss: 1.4026\n",
            "Epoch 69/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7144 - loss: 2.1034 - note_outputs_loss: 1.3889\n",
            "Epoch 70/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7066 - loss: 2.1151 - note_outputs_loss: 1.4085\n",
            "Epoch 71/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7172 - loss: 2.0970 - note_outputs_loss: 1.3798\n",
            "Epoch 72/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6934 - loss: 2.0861 - note_outputs_loss: 1.3927\n",
            "Epoch 73/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7268 - loss: 2.1012 - note_outputs_loss: 1.3744\n",
            "Epoch 74/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7138 - loss: 2.0744 - note_outputs_loss: 1.3606\n",
            "Epoch 75/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7181 - loss: 2.1024 - note_outputs_loss: 1.3842\n",
            "Epoch 76/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7273 - loss: 2.0952 - note_outputs_loss: 1.3679\n",
            "Epoch 77/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7113 - loss: 2.0851 - note_outputs_loss: 1.3738\n",
            "Epoch 78/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6965 - loss: 2.0465 - note_outputs_loss: 1.3500\n",
            "Epoch 79/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7045 - loss: 2.0627 - note_outputs_loss: 1.3582\n",
            "Epoch 80/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6940 - loss: 2.0337 - note_outputs_loss: 1.3397\n",
            "Epoch 81/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6997 - loss: 2.0598 - note_outputs_loss: 1.3601\n",
            "Epoch 82/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7352 - loss: 2.0743 - note_outputs_loss: 1.3392\n",
            "Epoch 83/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6899 - loss: 2.0330 - note_outputs_loss: 1.3432\n",
            "Epoch 84/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7363 - loss: 2.0741 - note_outputs_loss: 1.3378\n",
            "Epoch 85/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7369 - loss: 2.0855 - note_outputs_loss: 1.3486\n",
            "Epoch 86/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7175 - loss: 2.0786 - note_outputs_loss: 1.3611\n",
            "Epoch 87/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - duration_outputs_loss: 0.7325 - loss: 2.0577 - note_outputs_loss: 1.3252\n",
            "Epoch 88/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7176 - loss: 2.0626 - note_outputs_loss: 1.3450\n",
            "Epoch 89/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7368 - loss: 2.0530 - note_outputs_loss: 1.3162\n",
            "Epoch 90/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7245 - loss: 2.0385 - note_outputs_loss: 1.3140\n",
            "Epoch 91/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - duration_outputs_loss: 0.7189 - loss: 2.0587 - note_outputs_loss: 1.3397\n",
            "Epoch 92/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - duration_outputs_loss: 0.7253 - loss: 2.0791 - note_outputs_loss: 1.3538\n",
            "Epoch 93/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7256 - loss: 2.0523 - note_outputs_loss: 1.3267\n",
            "Epoch 94/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7165 - loss: 2.0213 - note_outputs_loss: 1.3047\n",
            "Epoch 95/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - duration_outputs_loss: 0.7049 - loss: 2.0123 - note_outputs_loss: 1.3074\n",
            "Epoch 96/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - duration_outputs_loss: 0.7246 - loss: 2.0612 - note_outputs_loss: 1.3366\n",
            "Epoch 97/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7129 - loss: 2.0161 - note_outputs_loss: 1.3032\n",
            "Epoch 98/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7449 - loss: 2.0506 - note_outputs_loss: 1.3057\n",
            "Epoch 99/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6765 - loss: 1.9950 - note_outputs_loss: 1.3186\n",
            "Epoch 100/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7234 - loss: 2.0310 - note_outputs_loss: 1.3076\n",
            "Epoch 101/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7123 - loss: 2.0270 - note_outputs_loss: 1.3147\n",
            "Epoch 102/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7174 - loss: 2.0357 - note_outputs_loss: 1.3183\n",
            "Epoch 103/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7235 - loss: 2.0166 - note_outputs_loss: 1.2931\n",
            "Epoch 104/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6889 - loss: 1.9907 - note_outputs_loss: 1.3018\n",
            "Epoch 105/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7306 - loss: 2.0319 - note_outputs_loss: 1.3013\n",
            "Epoch 106/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6883 - loss: 1.9672 - note_outputs_loss: 1.2790\n",
            "Epoch 107/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.6936 - loss: 1.9876 - note_outputs_loss: 1.2940\n",
            "Epoch 108/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7145 - loss: 2.0015 - note_outputs_loss: 1.2870\n",
            "Epoch 109/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7238 - loss: 1.9782 - note_outputs_loss: 1.2545\n",
            "Epoch 110/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7371 - loss: 2.0291 - note_outputs_loss: 1.2920\n",
            "Epoch 111/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6989 - loss: 1.9832 - note_outputs_loss: 1.2843\n",
            "Epoch 112/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7073 - loss: 1.9922 - note_outputs_loss: 1.2849\n",
            "Epoch 113/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7300 - loss: 2.0020 - note_outputs_loss: 1.2720\n",
            "Epoch 114/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7349 - loss: 2.0213 - note_outputs_loss: 1.2864\n",
            "Epoch 115/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7568 - loss: 2.0416 - note_outputs_loss: 1.2848\n",
            "Epoch 116/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7265 - loss: 2.0129 - note_outputs_loss: 1.2864\n",
            "Epoch 117/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7089 - loss: 1.9625 - note_outputs_loss: 1.2536\n",
            "Epoch 118/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7219 - loss: 1.9992 - note_outputs_loss: 1.2772\n",
            "Epoch 119/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7402 - loss: 2.0120 - note_outputs_loss: 1.2717\n",
            "Epoch 120/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7051 - loss: 1.9606 - note_outputs_loss: 1.2556\n",
            "Epoch 121/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7308 - loss: 1.9911 - note_outputs_loss: 1.2604\n",
            "Epoch 122/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7078 - loss: 1.9656 - note_outputs_loss: 1.2578\n",
            "Epoch 123/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6844 - loss: 1.9619 - note_outputs_loss: 1.2775\n",
            "Epoch 124/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7024 - loss: 1.9610 - note_outputs_loss: 1.2586\n",
            "Epoch 125/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7298 - loss: 1.9754 - note_outputs_loss: 1.2456\n",
            "Epoch 126/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7326 - loss: 1.9772 - note_outputs_loss: 1.2446\n",
            "Epoch 127/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7268 - loss: 1.9844 - note_outputs_loss: 1.2576\n",
            "Epoch 128/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - duration_outputs_loss: 0.6851 - loss: 1.9253 - note_outputs_loss: 1.2402\n",
            "Epoch 129/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7230 - loss: 1.9724 - note_outputs_loss: 1.2494\n",
            "Epoch 130/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7125 - loss: 1.9531 - note_outputs_loss: 1.2406\n",
            "Epoch 131/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7267 - loss: 1.9661 - note_outputs_loss: 1.2393\n",
            "Epoch 132/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7308 - loss: 1.9568 - note_outputs_loss: 1.2260\n",
            "Epoch 133/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7353 - loss: 1.9945 - note_outputs_loss: 1.2592\n",
            "Epoch 134/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6871 - loss: 1.9450 - note_outputs_loss: 1.2579\n",
            "Epoch 135/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6993 - loss: 1.9312 - note_outputs_loss: 1.2319\n",
            "Epoch 136/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7044 - loss: 1.9386 - note_outputs_loss: 1.2342\n",
            "Epoch 137/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7170 - loss: 1.9440 - note_outputs_loss: 1.2270\n",
            "Epoch 138/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7281 - loss: 1.9580 - note_outputs_loss: 1.2299\n",
            "Epoch 139/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7149 - loss: 1.9375 - note_outputs_loss: 1.2227\n",
            "Epoch 140/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7130 - loss: 1.9549 - note_outputs_loss: 1.2419\n",
            "Epoch 141/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7413 - loss: 1.9675 - note_outputs_loss: 1.2262\n",
            "Epoch 142/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7014 - loss: 1.9400 - note_outputs_loss: 1.2386\n",
            "Epoch 143/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7072 - loss: 1.9228 - note_outputs_loss: 1.2156\n",
            "Epoch 144/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7120 - loss: 1.9311 - note_outputs_loss: 1.2192\n",
            "Epoch 145/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7152 - loss: 1.9320 - note_outputs_loss: 1.2168\n",
            "Epoch 146/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7098 - loss: 1.9195 - note_outputs_loss: 1.2097\n",
            "Epoch 147/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7425 - loss: 1.9644 - note_outputs_loss: 1.2219\n",
            "Epoch 148/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7105 - loss: 1.9244 - note_outputs_loss: 1.2139\n",
            "Epoch 149/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7186 - loss: 1.9326 - note_outputs_loss: 1.2140\n",
            "Epoch 150/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7197 - loss: 1.9303 - note_outputs_loss: 1.2106\n",
            "Epoch 151/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7160 - loss: 1.9162 - note_outputs_loss: 1.2002\n",
            "Epoch 152/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7198 - loss: 1.9501 - note_outputs_loss: 1.2302\n",
            "Epoch 153/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7343 - loss: 1.9565 - note_outputs_loss: 1.2221\n",
            "Epoch 154/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.6921 - loss: 1.9029 - note_outputs_loss: 1.2108\n",
            "Epoch 155/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7209 - loss: 1.9411 - note_outputs_loss: 1.2202\n",
            "Epoch 156/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7114 - loss: 1.9215 - note_outputs_loss: 1.2101\n",
            "Epoch 157/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7159 - loss: 1.9374 - note_outputs_loss: 1.2215\n",
            "Epoch 158/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7121 - loss: 1.9328 - note_outputs_loss: 1.2207\n",
            "Epoch 159/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7066 - loss: 1.9204 - note_outputs_loss: 1.2138\n",
            "Epoch 160/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7056 - loss: 1.9241 - note_outputs_loss: 1.2185\n",
            "Epoch 161/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7173 - loss: 1.9210 - note_outputs_loss: 1.2036\n",
            "Epoch 162/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7208 - loss: 1.9199 - note_outputs_loss: 1.1991\n",
            "Epoch 163/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7261 - loss: 1.9205 - note_outputs_loss: 1.1944\n",
            "Epoch 164/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7405 - loss: 1.9623 - note_outputs_loss: 1.2218\n",
            "Epoch 165/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6991 - loss: 1.8995 - note_outputs_loss: 1.2004\n",
            "Epoch 166/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7148 - loss: 1.9044 - note_outputs_loss: 1.1896\n",
            "Epoch 167/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7326 - loss: 1.9172 - note_outputs_loss: 1.1845\n",
            "Epoch 168/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7299 - loss: 1.9083 - note_outputs_loss: 1.1784\n",
            "Epoch 169/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7128 - loss: 1.9212 - note_outputs_loss: 1.2085\n",
            "Epoch 170/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7307 - loss: 1.9374 - note_outputs_loss: 1.2067\n",
            "Epoch 171/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7156 - loss: 1.8954 - note_outputs_loss: 1.1798\n",
            "Epoch 172/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - duration_outputs_loss: 0.6916 - loss: 1.8874 - note_outputs_loss: 1.1958\n",
            "Epoch 173/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7139 - loss: 1.9100 - note_outputs_loss: 1.1961\n",
            "Epoch 174/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - duration_outputs_loss: 0.7271 - loss: 1.9228 - note_outputs_loss: 1.1957\n",
            "Epoch 175/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7125 - loss: 1.8823 - note_outputs_loss: 1.1698\n",
            "Epoch 176/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7129 - loss: 1.8838 - note_outputs_loss: 1.1709\n",
            "Epoch 177/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7274 - loss: 1.8904 - note_outputs_loss: 1.1630\n",
            "Epoch 178/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7039 - loss: 1.8786 - note_outputs_loss: 1.1747\n",
            "Epoch 179/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7188 - loss: 1.9151 - note_outputs_loss: 1.1963\n",
            "Epoch 180/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7155 - loss: 1.9015 - note_outputs_loss: 1.1861\n",
            "Epoch 181/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7142 - loss: 1.8859 - note_outputs_loss: 1.1717\n",
            "Epoch 182/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7318 - loss: 1.8971 - note_outputs_loss: 1.1653\n",
            "Epoch 183/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7043 - loss: 1.8952 - note_outputs_loss: 1.1909\n",
            "Epoch 184/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7177 - loss: 1.9146 - note_outputs_loss: 1.1970\n",
            "Epoch 185/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7344 - loss: 1.8924 - note_outputs_loss: 1.1581\n",
            "Epoch 186/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7146 - loss: 1.8878 - note_outputs_loss: 1.1732\n",
            "Epoch 187/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7383 - loss: 1.8970 - note_outputs_loss: 1.1588\n",
            "Epoch 188/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 22ms/step - duration_outputs_loss: 0.7086 - loss: 1.8831 - note_outputs_loss: 1.1745\n",
            "Epoch 189/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7281 - loss: 1.8926 - note_outputs_loss: 1.1644\n",
            "Epoch 190/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7339 - loss: 1.9228 - note_outputs_loss: 1.1889\n",
            "Epoch 191/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7162 - loss: 1.8942 - note_outputs_loss: 1.1780\n",
            "Epoch 192/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7235 - loss: 1.8783 - note_outputs_loss: 1.1548\n",
            "Epoch 193/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7163 - loss: 1.9046 - note_outputs_loss: 1.1882\n",
            "Epoch 194/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7339 - loss: 1.9004 - note_outputs_loss: 1.1665\n",
            "Epoch 195/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7113 - loss: 1.8922 - note_outputs_loss: 1.1810\n",
            "Epoch 196/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.6941 - loss: 1.8648 - note_outputs_loss: 1.1707\n",
            "Epoch 197/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7337 - loss: 1.8977 - note_outputs_loss: 1.1640\n",
            "Epoch 198/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7280 - loss: 1.8904 - note_outputs_loss: 1.1624\n",
            "Epoch 199/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7052 - loss: 1.8751 - note_outputs_loss: 1.1698\n",
            "Epoch 200/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7032 - loss: 1.8549 - note_outputs_loss: 1.1517\n",
            "Epoch 201/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7040 - loss: 1.8652 - note_outputs_loss: 1.1612\n",
            "Epoch 202/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7060 - loss: 1.8516 - note_outputs_loss: 1.1457\n",
            "Epoch 203/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7143 - loss: 1.8629 - note_outputs_loss: 1.1486\n",
            "Epoch 204/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.6928 - loss: 1.8455 - note_outputs_loss: 1.1527\n",
            "Epoch 205/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7030 - loss: 1.8565 - note_outputs_loss: 1.1535\n",
            "Epoch 206/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7086 - loss: 1.8686 - note_outputs_loss: 1.1600\n",
            "Epoch 207/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7121 - loss: 1.8919 - note_outputs_loss: 1.1798\n",
            "Epoch 208/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7048 - loss: 1.8568 - note_outputs_loss: 1.1519\n",
            "Epoch 209/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7281 - loss: 1.8724 - note_outputs_loss: 1.1443\n",
            "Epoch 210/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.6980 - loss: 1.8536 - note_outputs_loss: 1.1556\n",
            "Epoch 211/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7160 - loss: 1.8697 - note_outputs_loss: 1.1537\n",
            "Epoch 212/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7271 - loss: 1.9024 - note_outputs_loss: 1.1753\n",
            "Epoch 213/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7117 - loss: 1.8600 - note_outputs_loss: 1.1482\n",
            "Epoch 214/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7082 - loss: 1.8533 - note_outputs_loss: 1.1451\n",
            "Epoch 215/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6979 - loss: 1.8495 - note_outputs_loss: 1.1517\n",
            "Epoch 216/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7287 - loss: 1.8611 - note_outputs_loss: 1.1324\n",
            "Epoch 217/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7226 - loss: 1.8773 - note_outputs_loss: 1.1547\n",
            "Epoch 218/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7256 - loss: 1.8700 - note_outputs_loss: 1.1444\n",
            "Epoch 219/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6926 - loss: 1.8193 - note_outputs_loss: 1.1267\n",
            "Epoch 220/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - duration_outputs_loss: 0.6942 - loss: 1.8384 - note_outputs_loss: 1.1442\n",
            "Epoch 221/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7063 - loss: 1.8413 - note_outputs_loss: 1.1349\n",
            "Epoch 222/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7233 - loss: 1.8723 - note_outputs_loss: 1.1490\n",
            "Epoch 223/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7397 - loss: 1.8913 - note_outputs_loss: 1.1516\n",
            "Epoch 224/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7294 - loss: 1.8780 - note_outputs_loss: 1.1486\n",
            "Epoch 225/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7283 - loss: 1.8662 - note_outputs_loss: 1.1379\n",
            "Epoch 226/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6960 - loss: 1.8318 - note_outputs_loss: 1.1358\n",
            "Epoch 227/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7041 - loss: 1.8518 - note_outputs_loss: 1.1477\n",
            "Epoch 228/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7175 - loss: 1.8655 - note_outputs_loss: 1.1480\n",
            "Epoch 229/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7028 - loss: 1.8461 - note_outputs_loss: 1.1434\n",
            "Epoch 230/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7160 - loss: 1.8484 - note_outputs_loss: 1.1324\n",
            "Epoch 231/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7231 - loss: 1.8642 - note_outputs_loss: 1.1411\n",
            "Epoch 232/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7219 - loss: 1.8468 - note_outputs_loss: 1.1249\n",
            "Epoch 233/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7068 - loss: 1.8339 - note_outputs_loss: 1.1271\n",
            "Epoch 234/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7215 - loss: 1.8661 - note_outputs_loss: 1.1446\n",
            "Epoch 235/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7425 - loss: 1.8828 - note_outputs_loss: 1.1403\n",
            "Epoch 236/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7183 - loss: 1.8499 - note_outputs_loss: 1.1316\n",
            "Epoch 237/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7317 - loss: 1.8666 - note_outputs_loss: 1.1349\n",
            "Epoch 238/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7438 - loss: 1.8796 - note_outputs_loss: 1.1358\n",
            "Epoch 239/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7212 - loss: 1.8646 - note_outputs_loss: 1.1434\n",
            "Epoch 240/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7270 - loss: 1.8523 - note_outputs_loss: 1.1254\n",
            "Epoch 241/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7156 - loss: 1.8608 - note_outputs_loss: 1.1453\n",
            "Epoch 242/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7235 - loss: 1.8501 - note_outputs_loss: 1.1266\n",
            "Epoch 243/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - duration_outputs_loss: 0.7060 - loss: 1.8355 - note_outputs_loss: 1.1295\n",
            "Epoch 244/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7172 - loss: 1.8533 - note_outputs_loss: 1.1361\n",
            "Epoch 245/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7216 - loss: 1.8481 - note_outputs_loss: 1.1265\n",
            "Epoch 246/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7022 - loss: 1.8382 - note_outputs_loss: 1.1360\n",
            "Epoch 247/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7465 - loss: 1.8715 - note_outputs_loss: 1.1251\n",
            "Epoch 248/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7073 - loss: 1.8266 - note_outputs_loss: 1.1194\n",
            "Epoch 249/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7200 - loss: 1.8521 - note_outputs_loss: 1.1321\n",
            "Epoch 250/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7388 - loss: 1.8759 - note_outputs_loss: 1.1371\n",
            "Epoch 251/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7236 - loss: 1.8563 - note_outputs_loss: 1.1327\n",
            "Epoch 252/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7150 - loss: 1.8629 - note_outputs_loss: 1.1478\n",
            "Epoch 253/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - duration_outputs_loss: 0.7235 - loss: 1.8467 - note_outputs_loss: 1.1231\n",
            "Epoch 254/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7222 - loss: 1.8222 - note_outputs_loss: 1.1000\n",
            "Epoch 255/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7294 - loss: 1.8681 - note_outputs_loss: 1.1387\n",
            "Epoch 256/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7304 - loss: 1.8449 - note_outputs_loss: 1.1145\n",
            "Epoch 257/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7118 - loss: 1.8349 - note_outputs_loss: 1.1231\n",
            "Epoch 258/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7148 - loss: 1.8237 - note_outputs_loss: 1.1089\n",
            "Epoch 259/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7070 - loss: 1.8229 - note_outputs_loss: 1.1159\n",
            "Epoch 260/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7320 - loss: 1.8708 - note_outputs_loss: 1.1389\n",
            "Epoch 261/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7117 - loss: 1.8372 - note_outputs_loss: 1.1254\n",
            "Epoch 262/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7147 - loss: 1.8265 - note_outputs_loss: 1.1118\n",
            "Epoch 263/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7160 - loss: 1.8334 - note_outputs_loss: 1.1173\n",
            "Epoch 264/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7180 - loss: 1.8243 - note_outputs_loss: 1.1063\n",
            "Epoch 265/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7217 - loss: 1.8407 - note_outputs_loss: 1.1190\n",
            "Epoch 266/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7303 - loss: 1.8361 - note_outputs_loss: 1.1058\n",
            "Epoch 267/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7143 - loss: 1.8241 - note_outputs_loss: 1.1098\n",
            "Epoch 268/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7050 - loss: 1.8176 - note_outputs_loss: 1.1125\n",
            "Epoch 269/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7023 - loss: 1.7995 - note_outputs_loss: 1.0972\n",
            "Epoch 270/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7095 - loss: 1.8136 - note_outputs_loss: 1.1041\n",
            "Epoch 271/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7005 - loss: 1.8025 - note_outputs_loss: 1.1019\n",
            "Epoch 272/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6960 - loss: 1.8089 - note_outputs_loss: 1.1129\n",
            "Epoch 273/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7050 - loss: 1.8231 - note_outputs_loss: 1.1181\n",
            "Epoch 274/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7373 - loss: 1.8574 - note_outputs_loss: 1.1201\n",
            "Epoch 275/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7154 - loss: 1.8324 - note_outputs_loss: 1.1170\n",
            "Epoch 276/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.6994 - loss: 1.7950 - note_outputs_loss: 1.0956\n",
            "Epoch 277/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7213 - loss: 1.8138 - note_outputs_loss: 1.0925\n",
            "Epoch 278/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7265 - loss: 1.8476 - note_outputs_loss: 1.1211\n",
            "Epoch 279/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7370 - loss: 1.8463 - note_outputs_loss: 1.1093\n",
            "Epoch 280/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7039 - loss: 1.8011 - note_outputs_loss: 1.0973\n",
            "Epoch 281/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7400 - loss: 1.8527 - note_outputs_loss: 1.1127\n",
            "Epoch 282/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7025 - loss: 1.7908 - note_outputs_loss: 1.0884\n",
            "Epoch 283/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7188 - loss: 1.8359 - note_outputs_loss: 1.1171\n",
            "Epoch 284/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7335 - loss: 1.8285 - note_outputs_loss: 1.0950\n",
            "Epoch 285/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7053 - loss: 1.8070 - note_outputs_loss: 1.1017\n",
            "Epoch 286/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6825 - loss: 1.7608 - note_outputs_loss: 1.0783\n",
            "Epoch 287/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7124 - loss: 1.8181 - note_outputs_loss: 1.1057\n",
            "Epoch 288/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7215 - loss: 1.8214 - note_outputs_loss: 1.1000\n",
            "Epoch 289/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7093 - loss: 1.8125 - note_outputs_loss: 1.1032\n",
            "Epoch 290/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7068 - loss: 1.7981 - note_outputs_loss: 1.0913\n",
            "Epoch 291/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7207 - loss: 1.8026 - note_outputs_loss: 1.0819\n",
            "Epoch 292/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7295 - loss: 1.8203 - note_outputs_loss: 1.0908\n",
            "Epoch 293/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7027 - loss: 1.7965 - note_outputs_loss: 1.0938\n",
            "Epoch 294/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7355 - loss: 1.8431 - note_outputs_loss: 1.1076\n",
            "Epoch 295/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - duration_outputs_loss: 0.7122 - loss: 1.7941 - note_outputs_loss: 1.0819\n",
            "Epoch 296/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7141 - loss: 1.8156 - note_outputs_loss: 1.1015\n",
            "Epoch 297/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7137 - loss: 1.8015 - note_outputs_loss: 1.0877\n",
            "Epoch 298/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7150 - loss: 1.8005 - note_outputs_loss: 1.0855\n",
            "Epoch 299/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 22ms/step - duration_outputs_loss: 0.7005 - loss: 1.7935 - note_outputs_loss: 1.0931\n",
            "Epoch 300/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7087 - loss: 1.7909 - note_outputs_loss: 1.0822\n",
            "Epoch 301/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6982 - loss: 1.8015 - note_outputs_loss: 1.1032\n",
            "Epoch 302/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6979 - loss: 1.7919 - note_outputs_loss: 1.0941\n",
            "Epoch 303/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7217 - loss: 1.8130 - note_outputs_loss: 1.0913\n",
            "Epoch 304/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7042 - loss: 1.7805 - note_outputs_loss: 1.0762\n",
            "Epoch 305/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7271 - loss: 1.8289 - note_outputs_loss: 1.1018\n",
            "Epoch 306/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6951 - loss: 1.7739 - note_outputs_loss: 1.0789\n",
            "Epoch 307/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7179 - loss: 1.8029 - note_outputs_loss: 1.0850\n",
            "Epoch 308/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 20ms/step - duration_outputs_loss: 0.7112 - loss: 1.8153 - note_outputs_loss: 1.1042\n",
            "Epoch 309/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7003 - loss: 1.7822 - note_outputs_loss: 1.0819\n",
            "Epoch 310/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7338 - loss: 1.8073 - note_outputs_loss: 1.0735\n",
            "Epoch 311/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7132 - loss: 1.7999 - note_outputs_loss: 1.0867\n",
            "Epoch 312/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7128 - loss: 1.8036 - note_outputs_loss: 1.0908\n",
            "Epoch 313/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7248 - loss: 1.8193 - note_outputs_loss: 1.0946\n",
            "Epoch 314/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7057 - loss: 1.7955 - note_outputs_loss: 1.0898\n",
            "Epoch 315/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7479 - loss: 1.8384 - note_outputs_loss: 1.0905\n",
            "Epoch 316/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7133 - loss: 1.7883 - note_outputs_loss: 1.0751\n",
            "Epoch 317/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7032 - loss: 1.7785 - note_outputs_loss: 1.0753\n",
            "Epoch 318/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7184 - loss: 1.8070 - note_outputs_loss: 1.0886\n",
            "Epoch 319/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 20ms/step - duration_outputs_loss: 0.7031 - loss: 1.7903 - note_outputs_loss: 1.0871\n",
            "Epoch 320/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - duration_outputs_loss: 0.6926 - loss: 1.7675 - note_outputs_loss: 1.0749\n",
            "Epoch 321/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7048 - loss: 1.7995 - note_outputs_loss: 1.0947\n",
            "Epoch 322/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7183 - loss: 1.8015 - note_outputs_loss: 1.0832\n",
            "Epoch 323/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7173 - loss: 1.8009 - note_outputs_loss: 1.0836\n",
            "Epoch 324/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6927 - loss: 1.7623 - note_outputs_loss: 1.0696\n",
            "Epoch 325/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7074 - loss: 1.7781 - note_outputs_loss: 1.0708\n",
            "Epoch 326/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7237 - loss: 1.8091 - note_outputs_loss: 1.0855\n",
            "Epoch 327/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7308 - loss: 1.7995 - note_outputs_loss: 1.0687\n",
            "Epoch 328/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 20ms/step - duration_outputs_loss: 0.7394 - loss: 1.8041 - note_outputs_loss: 1.0646\n",
            "Epoch 329/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6926 - loss: 1.7793 - note_outputs_loss: 1.0867\n",
            "Epoch 330/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7434 - loss: 1.8338 - note_outputs_loss: 1.0903\n",
            "Epoch 331/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7006 - loss: 1.7690 - note_outputs_loss: 1.0684\n",
            "Epoch 332/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - duration_outputs_loss: 0.7147 - loss: 1.7862 - note_outputs_loss: 1.0715\n",
            "Epoch 333/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7104 - loss: 1.8004 - note_outputs_loss: 1.0901\n",
            "Epoch 334/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7050 - loss: 1.7780 - note_outputs_loss: 1.0730\n",
            "Epoch 335/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7260 - loss: 1.8007 - note_outputs_loss: 1.0747\n",
            "Epoch 336/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7131 - loss: 1.8108 - note_outputs_loss: 1.0977\n",
            "Epoch 337/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - duration_outputs_loss: 0.7244 - loss: 1.8057 - note_outputs_loss: 1.0813\n",
            "Epoch 338/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7194 - loss: 1.7784 - note_outputs_loss: 1.0590\n",
            "Epoch 339/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7375 - loss: 1.8153 - note_outputs_loss: 1.0777\n",
            "Epoch 340/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7069 - loss: 1.7810 - note_outputs_loss: 1.0741\n",
            "Epoch 341/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7007 - loss: 1.7671 - note_outputs_loss: 1.0664\n",
            "Epoch 342/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7254 - loss: 1.8038 - note_outputs_loss: 1.0784\n",
            "Epoch 343/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7186 - loss: 1.8011 - note_outputs_loss: 1.0825\n",
            "Epoch 344/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7077 - loss: 1.7860 - note_outputs_loss: 1.0784\n",
            "Epoch 345/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7315 - loss: 1.7969 - note_outputs_loss: 1.0654\n",
            "Epoch 346/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7144 - loss: 1.8020 - note_outputs_loss: 1.0876\n",
            "Epoch 347/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7164 - loss: 1.7899 - note_outputs_loss: 1.0734\n",
            "Epoch 348/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7185 - loss: 1.7795 - note_outputs_loss: 1.0610\n",
            "Epoch 349/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7298 - loss: 1.7925 - note_outputs_loss: 1.0627\n",
            "Epoch 350/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7215 - loss: 1.7913 - note_outputs_loss: 1.0698\n",
            "Epoch 351/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7078 - loss: 1.7925 - note_outputs_loss: 1.0847\n",
            "Epoch 352/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7366 - loss: 1.8081 - note_outputs_loss: 1.0716\n",
            "Epoch 353/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6951 - loss: 1.7719 - note_outputs_loss: 1.0768\n",
            "Epoch 354/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7227 - loss: 1.7948 - note_outputs_loss: 1.0721\n",
            "Epoch 355/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7384 - loss: 1.8206 - note_outputs_loss: 1.0822\n",
            "Epoch 356/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7104 - loss: 1.7844 - note_outputs_loss: 1.0740\n",
            "Epoch 357/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7040 - loss: 1.7601 - note_outputs_loss: 1.0561\n",
            "Epoch 358/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7274 - loss: 1.8049 - note_outputs_loss: 1.0775\n",
            "Epoch 359/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6958 - loss: 1.7698 - note_outputs_loss: 1.0740\n",
            "Epoch 360/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7250 - loss: 1.7716 - note_outputs_loss: 1.0466\n",
            "Epoch 361/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7271 - loss: 1.7855 - note_outputs_loss: 1.0583\n",
            "Epoch 362/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7286 - loss: 1.7763 - note_outputs_loss: 1.0477\n",
            "Epoch 363/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7168 - loss: 1.7846 - note_outputs_loss: 1.0678\n",
            "Epoch 364/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7312 - loss: 1.8012 - note_outputs_loss: 1.0700\n",
            "Epoch 365/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7180 - loss: 1.7753 - note_outputs_loss: 1.0573\n",
            "Epoch 366/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7245 - loss: 1.7713 - note_outputs_loss: 1.0468\n",
            "Epoch 367/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7075 - loss: 1.7810 - note_outputs_loss: 1.0734\n",
            "Epoch 368/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7024 - loss: 1.7583 - note_outputs_loss: 1.0559\n",
            "Epoch 369/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7388 - loss: 1.7811 - note_outputs_loss: 1.0424\n",
            "Epoch 370/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7001 - loss: 1.7698 - note_outputs_loss: 1.0696\n",
            "Epoch 371/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7234 - loss: 1.7714 - note_outputs_loss: 1.0480\n",
            "Epoch 372/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7242 - loss: 1.7766 - note_outputs_loss: 1.0525\n",
            "Epoch 373/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7040 - loss: 1.7709 - note_outputs_loss: 1.0669\n",
            "Epoch 374/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7233 - loss: 1.7867 - note_outputs_loss: 1.0634\n",
            "Epoch 375/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.6998 - loss: 1.7387 - note_outputs_loss: 1.0390\n",
            "Epoch 376/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7238 - loss: 1.7817 - note_outputs_loss: 1.0579\n",
            "Epoch 377/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7347 - loss: 1.7683 - note_outputs_loss: 1.0335\n",
            "Epoch 378/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7035 - loss: 1.7763 - note_outputs_loss: 1.0728\n",
            "Epoch 379/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.6946 - loss: 1.7557 - note_outputs_loss: 1.0611\n",
            "Epoch 380/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7321 - loss: 1.7759 - note_outputs_loss: 1.0438\n",
            "Epoch 381/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7317 - loss: 1.7880 - note_outputs_loss: 1.0564\n",
            "Epoch 382/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.6882 - loss: 1.7364 - note_outputs_loss: 1.0481\n",
            "Epoch 383/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - duration_outputs_loss: 0.7311 - loss: 1.7940 - note_outputs_loss: 1.0629\n",
            "Epoch 384/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7215 - loss: 1.7820 - note_outputs_loss: 1.0605\n",
            "Epoch 385/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7010 - loss: 1.7350 - note_outputs_loss: 1.0339\n",
            "Epoch 386/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7195 - loss: 1.7716 - note_outputs_loss: 1.0520\n",
            "Epoch 387/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7108 - loss: 1.7614 - note_outputs_loss: 1.0506\n",
            "Epoch 388/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7260 - loss: 1.7698 - note_outputs_loss: 1.0438\n",
            "Epoch 389/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7291 - loss: 1.7740 - note_outputs_loss: 1.0449\n",
            "Epoch 390/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.6950 - loss: 1.7509 - note_outputs_loss: 1.0559\n",
            "Epoch 391/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7127 - loss: 1.7405 - note_outputs_loss: 1.0278\n",
            "Epoch 392/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7248 - loss: 1.7765 - note_outputs_loss: 1.0517\n",
            "Epoch 393/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7230 - loss: 1.7810 - note_outputs_loss: 1.0580\n",
            "Epoch 394/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7160 - loss: 1.7815 - note_outputs_loss: 1.0655\n",
            "Epoch 395/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7038 - loss: 1.7734 - note_outputs_loss: 1.0696\n",
            "Epoch 396/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 20ms/step - duration_outputs_loss: 0.6956 - loss: 1.7444 - note_outputs_loss: 1.0488\n",
            "Epoch 397/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7039 - loss: 1.7522 - note_outputs_loss: 1.0482\n",
            "Epoch 398/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7153 - loss: 1.7556 - note_outputs_loss: 1.0402\n",
            "Epoch 399/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7213 - loss: 1.7557 - note_outputs_loss: 1.0343\n",
            "Epoch 400/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7414 - loss: 1.7990 - note_outputs_loss: 1.0576\n",
            "Epoch 401/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7009 - loss: 1.7530 - note_outputs_loss: 1.0520\n",
            "Epoch 402/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7178 - loss: 1.7757 - note_outputs_loss: 1.0578\n",
            "Epoch 403/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7184 - loss: 1.7555 - note_outputs_loss: 1.0371\n",
            "Epoch 404/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6969 - loss: 1.7476 - note_outputs_loss: 1.0507\n",
            "Epoch 405/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7099 - loss: 1.7460 - note_outputs_loss: 1.0361\n",
            "Epoch 406/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7238 - loss: 1.7671 - note_outputs_loss: 1.0433\n",
            "Epoch 407/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7242 - loss: 1.7714 - note_outputs_loss: 1.0472\n",
            "Epoch 408/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7051 - loss: 1.7532 - note_outputs_loss: 1.0481\n",
            "Epoch 409/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7115 - loss: 1.7749 - note_outputs_loss: 1.0634\n",
            "Epoch 410/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7234 - loss: 1.7585 - note_outputs_loss: 1.0351\n",
            "Epoch 411/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7207 - loss: 1.7500 - note_outputs_loss: 1.0293\n",
            "Epoch 412/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7406 - loss: 1.7913 - note_outputs_loss: 1.0507\n",
            "Epoch 413/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7182 - loss: 1.7737 - note_outputs_loss: 1.0556\n",
            "Epoch 414/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7240 - loss: 1.7611 - note_outputs_loss: 1.0370\n",
            "Epoch 415/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7096 - loss: 1.7526 - note_outputs_loss: 1.0430\n",
            "Epoch 416/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7285 - loss: 1.7691 - note_outputs_loss: 1.0406\n",
            "Epoch 417/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7359 - loss: 1.7784 - note_outputs_loss: 1.0425\n",
            "Epoch 418/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7192 - loss: 1.7474 - note_outputs_loss: 1.0282\n",
            "Epoch 419/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7335 - loss: 1.7809 - note_outputs_loss: 1.0473\n",
            "Epoch 420/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7218 - loss: 1.7708 - note_outputs_loss: 1.0490\n",
            "Epoch 421/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - duration_outputs_loss: 0.7088 - loss: 1.7366 - note_outputs_loss: 1.0278\n",
            "Epoch 422/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7173 - loss: 1.7489 - note_outputs_loss: 1.0316\n",
            "Epoch 423/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7021 - loss: 1.7499 - note_outputs_loss: 1.0477\n",
            "Epoch 424/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7235 - loss: 1.7514 - note_outputs_loss: 1.0280\n",
            "Epoch 425/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6949 - loss: 1.7526 - note_outputs_loss: 1.0577\n",
            "Epoch 426/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7134 - loss: 1.7604 - note_outputs_loss: 1.0471\n",
            "Epoch 427/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6863 - loss: 1.7207 - note_outputs_loss: 1.0344\n",
            "Epoch 428/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7051 - loss: 1.7380 - note_outputs_loss: 1.0329\n",
            "Epoch 429/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7105 - loss: 1.7348 - note_outputs_loss: 1.0243\n",
            "Epoch 430/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6999 - loss: 1.7394 - note_outputs_loss: 1.0395\n",
            "Epoch 431/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7015 - loss: 1.7510 - note_outputs_loss: 1.0495\n",
            "Epoch 432/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7412 - loss: 1.7710 - note_outputs_loss: 1.0298\n",
            "Epoch 433/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7097 - loss: 1.7427 - note_outputs_loss: 1.0330\n",
            "Epoch 434/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7244 - loss: 1.7629 - note_outputs_loss: 1.0385\n",
            "Epoch 435/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7148 - loss: 1.7526 - note_outputs_loss: 1.0378\n",
            "Epoch 436/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - duration_outputs_loss: 0.7156 - loss: 1.7609 - note_outputs_loss: 1.0453\n",
            "Epoch 437/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7189 - loss: 1.7524 - note_outputs_loss: 1.0336\n",
            "Epoch 438/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7232 - loss: 1.7492 - note_outputs_loss: 1.0260\n",
            "Epoch 439/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7234 - loss: 1.7482 - note_outputs_loss: 1.0248\n",
            "Epoch 440/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7185 - loss: 1.7479 - note_outputs_loss: 1.0293\n",
            "Epoch 441/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7262 - loss: 1.7617 - note_outputs_loss: 1.0355\n",
            "Epoch 442/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.6904 - loss: 1.7036 - note_outputs_loss: 1.0132\n",
            "Epoch 443/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7157 - loss: 1.7559 - note_outputs_loss: 1.0402\n",
            "Epoch 444/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7042 - loss: 1.7380 - note_outputs_loss: 1.0338\n",
            "Epoch 445/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7296 - loss: 1.7511 - note_outputs_loss: 1.0216\n",
            "Epoch 446/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7031 - loss: 1.7480 - note_outputs_loss: 1.0448\n",
            "Epoch 447/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7349 - loss: 1.7673 - note_outputs_loss: 1.0324\n",
            "Epoch 448/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7268 - loss: 1.7502 - note_outputs_loss: 1.0234\n",
            "Epoch 449/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7172 - loss: 1.7583 - note_outputs_loss: 1.0411\n",
            "Epoch 450/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6978 - loss: 1.7478 - note_outputs_loss: 1.0500\n",
            "Epoch 451/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7159 - loss: 1.7589 - note_outputs_loss: 1.0430\n",
            "Epoch 452/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7223 - loss: 1.7472 - note_outputs_loss: 1.0250\n",
            "Epoch 453/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.6855 - loss: 1.7077 - note_outputs_loss: 1.0221\n",
            "Epoch 454/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7195 - loss: 1.7546 - note_outputs_loss: 1.0350\n",
            "Epoch 455/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7145 - loss: 1.7528 - note_outputs_loss: 1.0384\n",
            "Epoch 456/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7294 - loss: 1.7525 - note_outputs_loss: 1.0231\n",
            "Epoch 457/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7412 - loss: 1.7742 - note_outputs_loss: 1.0329\n",
            "Epoch 458/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7265 - loss: 1.7544 - note_outputs_loss: 1.0279\n",
            "Epoch 459/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7364 - loss: 1.7542 - note_outputs_loss: 1.0177\n",
            "Epoch 460/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7052 - loss: 1.7270 - note_outputs_loss: 1.0218\n",
            "Epoch 461/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7123 - loss: 1.7326 - note_outputs_loss: 1.0203\n",
            "Epoch 462/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7237 - loss: 1.7425 - note_outputs_loss: 1.0188\n",
            "Epoch 463/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6921 - loss: 1.7209 - note_outputs_loss: 1.0287\n",
            "Epoch 464/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7112 - loss: 1.7376 - note_outputs_loss: 1.0264\n",
            "Epoch 465/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7181 - loss: 1.7576 - note_outputs_loss: 1.0395\n",
            "Epoch 466/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7185 - loss: 1.7554 - note_outputs_loss: 1.0369\n",
            "Epoch 467/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7075 - loss: 1.7308 - note_outputs_loss: 1.0233\n",
            "Epoch 468/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7278 - loss: 1.7661 - note_outputs_loss: 1.0383\n",
            "Epoch 469/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7380 - loss: 1.7665 - note_outputs_loss: 1.0285\n",
            "Epoch 470/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7087 - loss: 1.7340 - note_outputs_loss: 1.0253\n",
            "Epoch 471/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7234 - loss: 1.7420 - note_outputs_loss: 1.0187\n",
            "Epoch 472/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6940 - loss: 1.6953 - note_outputs_loss: 1.0012\n",
            "Epoch 473/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7270 - loss: 1.7620 - note_outputs_loss: 1.0350\n",
            "Epoch 474/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7149 - loss: 1.7196 - note_outputs_loss: 1.0047\n",
            "Epoch 475/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7001 - loss: 1.7119 - note_outputs_loss: 1.0118\n",
            "Epoch 476/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.6990 - loss: 1.7149 - note_outputs_loss: 1.0159\n",
            "Epoch 477/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7094 - loss: 1.7326 - note_outputs_loss: 1.0231\n",
            "Epoch 478/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.6997 - loss: 1.7284 - note_outputs_loss: 1.0286\n",
            "Epoch 479/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7067 - loss: 1.7270 - note_outputs_loss: 1.0203\n",
            "Epoch 480/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.6936 - loss: 1.7147 - note_outputs_loss: 1.0211\n",
            "Epoch 481/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7344 - loss: 1.7574 - note_outputs_loss: 1.0230\n",
            "Epoch 482/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7157 - loss: 1.7277 - note_outputs_loss: 1.0120\n",
            "Epoch 483/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7119 - loss: 1.7230 - note_outputs_loss: 1.0110\n",
            "Epoch 484/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7276 - loss: 1.7533 - note_outputs_loss: 1.0256\n",
            "Epoch 485/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - duration_outputs_loss: 0.7134 - loss: 1.7348 - note_outputs_loss: 1.0215\n",
            "Epoch 486/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7377 - loss: 1.7691 - note_outputs_loss: 1.0314\n",
            "Epoch 487/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7327 - loss: 1.7634 - note_outputs_loss: 1.0306\n",
            "Epoch 488/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7032 - loss: 1.7087 - note_outputs_loss: 1.0054\n",
            "Epoch 489/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - duration_outputs_loss: 0.6978 - loss: 1.7082 - note_outputs_loss: 1.0104\n",
            "Epoch 490/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7322 - loss: 1.7462 - note_outputs_loss: 1.0139\n",
            "Epoch 491/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7108 - loss: 1.7182 - note_outputs_loss: 1.0074\n",
            "Epoch 492/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - duration_outputs_loss: 0.7263 - loss: 1.7413 - note_outputs_loss: 1.0151\n",
            "Epoch 493/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7393 - loss: 1.7710 - note_outputs_loss: 1.0318\n",
            "Epoch 494/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - duration_outputs_loss: 0.7328 - loss: 1.7486 - note_outputs_loss: 1.0158\n",
            "Epoch 495/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - duration_outputs_loss: 0.7103 - loss: 1.7279 - note_outputs_loss: 1.0176\n",
            "Epoch 496/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.6971 - loss: 1.7161 - note_outputs_loss: 1.0190\n",
            "Epoch 497/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - duration_outputs_loss: 0.7199 - loss: 1.7282 - note_outputs_loss: 1.0083\n",
            "Epoch 498/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7151 - loss: 1.7311 - note_outputs_loss: 1.0160\n",
            "Epoch 499/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - duration_outputs_loss: 0.7193 - loss: 1.7198 - note_outputs_loss: 1.0005\n",
            "Epoch 500/500\n",
            "\u001b[1m669/669\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - duration_outputs_loss: 0.7039 - loss: 1.7369 - note_outputs_loss: 1.0330\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a83bde56ce0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Music Generation"
      ],
      "metadata": {
        "id": "PrOy3-4YKf6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate Music using the Transformer\n",
        "\n",
        "info = music_generator.generate(\n",
        "    [\"START\"], [\"0.0\"], max_tokens=200, temperature=0.9\n",
        ")\n",
        "midi_stream = info[-1][\"midi\"].chordify()\n",
        "#midi_stream.show()"
      ],
      "metadata": {
        "id": "lMKxU73WKwZN"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Write Music to a MIDI File\n",
        "\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "midi_stream.write(\n",
        "    \"midi\",\n",
        "    fp=os.path.join(\n",
        "        \"/content/drive/My Drive/Colab Notebooks/GenAI/output_joplin/\",\n",
        "        \"output-\" + timestr + \".mid\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3sFtPF-7Kwbj",
        "outputId": "4aeba2c6-9c7c-4165-f7db-2c8551b8e197"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/GenAI/output_joplin/output-20241203-183558.mid'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}