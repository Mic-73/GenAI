{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXea4RAlvI8gYdczJuLuRt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mic-73/GenAI/blob/main/HW5/Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Author: Michael Wood\n",
        "\n",
        "Purpose: In this project we will develop an LSTM (Long Short-Term Memory) model to generate text.\n",
        "*   By training the model on various works, we will try to produce coherent and stylistically relevant text based on seed phrases (prompts).\n",
        "*   To improve the model's performance, we will explore the use of multiple training data, additional LSTM layers, and other optimizations.\n",
        "*   The quantity and quality of training data are crucial for achieving meaningful text generation; a larger and more diverse dataset allows the model to better capture the nuances and patterns of written text.\n",
        "\n",
        "Note: Initial code for importing and loading the data and setting up the multilayer LSTM model was taken from the assignment page on Canvas. Initial code for setting up training the model, tokenizing the data, and the initial textGenerator class was taken from the course's Github repo here: https://github.com/bforoura/GenAI/blob/main/Module5/recipe_lstm.ipynb. The code was modified to fit the assignment's requirements.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sdFgOQkn3NeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "2dCYlO2w3OZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Collection and Preparation"
      ],
      "metadata": {
        "id": "wMGdTGmc3Qql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "from os import stat_result\n",
        "\n",
        "# from Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import string\n",
        "\n",
        "# Natural Language Toolkit\n",
        "import nltk"
      ],
      "metadata": {
        "id": "XjHSFCu_3kI4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PARAMETERS\n",
        "\n",
        "TEMPERATURE = 1.0\n",
        "VOCAB_SIZE = 20000\n",
        "SEQ_LENGTH = 50\n",
        "MAX_LEN = 100\n",
        "EMBEDDING_DIM = 100\n",
        "N_UNITS = 64\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25"
      ],
      "metadata": {
        "id": "4ukBMNK23kLP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import and Clean the Data\n",
        "\n",
        "# URLS of Charles Dickens Works\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/cache/epub/730/pg730.txt\",     # Oliver Twist\n",
        "    \"https://www.gutenberg.org/cache/epub/98/pg98.txt\",       # A Tale of Two Cities\n",
        "    \"https://www.gutenberg.org/cache/epub/19337/pg19337.txt\"  # A Christmas Carol\n",
        "]\n",
        "\n",
        "# Initialize empty string\n",
        "all_text = \"\"\n",
        "\n",
        "# Starting phrase for a book\n",
        "start = r\"(?i)^.*?\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK.*?\\n\"\n",
        "\n",
        "# Ending phrase for a book\n",
        "end = r\"(?i)\\n\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK.*$\"\n",
        "\n",
        "# Download the books and clean them\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    text = response.text\n",
        "\n",
        "    # Remove metadata before the actual book content\n",
        "    text_cleaned = re.sub(start, \"\", text, flags=re.DOTALL)\n",
        "\n",
        "    # Remove metadata after the actual book content\n",
        "    text_cleaned = re.sub(end, \"\", text_cleaned, flags=re.DOTALL)\n",
        "\n",
        "    # Append the cleaned text to all_text\n",
        "    all_text += text_cleaned.strip() + \"\\n\\n\"  # Separate books by newlines\n",
        "\n",
        "# Pad the punctuation, trim white space\n",
        "all_text = re.sub(r'([.,!?()\";])', r' \\1 ', all_text)\n",
        "all_text = re.sub(r'\\s+', ' ', all_text)\n",
        "all_text = all_text.strip()\n",
        "\n",
        "# Save combined text to a single file\n",
        "with open(\"combined_dickens.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(all_text)"
      ],
      "metadata": {
        "id": "TtUuVIus3kNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tokenize the Data\n",
        "\n",
        "# Tokenize the text (Word Tokens)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([all_text])\n",
        "sequences = tokenizer.texts_to_sequences([all_text])[0]\n",
        "\n",
        "# Create input and output pairs for training\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(SEQ_LENGTH, len(sequences)):\n",
        "    X.append(sequences[i-SEQ_LENGTH:i])\n",
        "    y.append(sequences[i])\n",
        "\n",
        "X = pad_sequences(X, maxlen=SEQ_LENGTH)\n",
        "y = np.array(y)\n",
        "\n",
        "# Adjust vocab size\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1  # +1 for padding\n",
        "\n",
        "# index_to_word for text generation\n",
        "index_to_word = {i: word for word, i in tokenizer.word_index.items()}"
      ],
      "metadata": {
        "id": "ZSKiUAZ5OEWx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Initial LSTM Model Training"
      ],
      "metadata": {
        "id": "NLY3HRrM3S-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create the LSTM Model\n",
        "\n",
        "def make_LSTM_model():\n",
        "  model = tf.keras.Sequential()                                                  # Sequential\n",
        "  model.add(layers.Input(shape=(SEQ_LENGTH,), dtype=\"int32\"))                    # Input Layer\n",
        "  model.add(layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM))                         # Embedding Layer\n",
        "  model.add(layers.LSTM(N_UNITS))                                                # LSTM Layer\n",
        "  model.add(layers.Dense(VOCAB_SIZE, activation=\"softmax\"))                      # Output Layer\n",
        "  return model"
      ],
      "metadata": {
        "id": "A4FDHUPVFCk4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TextGenerationCallback Class\n",
        "\n",
        "class TextGenerationCallback(Callback):\n",
        "    def __init__(self, model, index_to_word, word_to_index, seq_length=SEQ_LENGTH, temperature=1.0, max_tokens=MAX_LEN):\n",
        "        super().__init__()\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = word_to_index\n",
        "        self.seq_length = seq_length\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def generate_text(self, start_prompt):\n",
        "      # Convert the start prompt to a sequence of indices\n",
        "      start_tokens = [self.word_to_index.get(word, 1) for word in start_prompt.split()]  # 1 is for unknown words\n",
        "\n",
        "      generated_text = start_prompt\n",
        "\n",
        "      for _ in range(self.max_tokens):\n",
        "\n",
        "          # Pad the input sequence to ensure it's of the correct length\n",
        "          padded_input = np.array([start_tokens[-self.seq_length:]])\n",
        "\n",
        "          # Predict the next word probabilities (in a 2d shape)\n",
        "          predictions = self.model.predict(padded_input, verbose=0)[0, :]\n",
        "\n",
        "          # Apply temperature and normalize\n",
        "          predictions = np.asarray(predictions).flatten()\n",
        "          predictions = np.log(predictions + 1e-10) / self.temperature\n",
        "          predictions = np.exp(predictions) / np.sum(np.exp(predictions))\n",
        "\n",
        "          # Sample from the probability distribution\n",
        "          next_token = np.random.choice(len(predictions), p=predictions)\n",
        "\n",
        "          # Append the predicted token to the sequence\n",
        "          start_tokens.append(next_token)\n",
        "\n",
        "          # Convert the token back to a word and add it to the generated text\n",
        "          generated_text += ' ' + self.index_to_word.get(next_token, '?')\n",
        "\n",
        "          # Stop if the end token is generated\n",
        "          if next_token == 0:\n",
        "              break\n",
        "\n",
        "      return generated_text\n",
        "\n",
        "    # Generate text after each training epoch\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # Charles Dicken start prompt example\n",
        "        start_prompt = \"It was the best of times\"\n",
        "\n",
        "        # Generate the text\n",
        "        generated_text = self.generate_text(start_prompt)\n",
        "        print(f\"\\nEpoch {epoch+1} Generated Text: {generated_text}\\n\")"
      ],
      "metadata": {
        "id": "J4bF-F9BY5GU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Establish the LSTM Model\n",
        "\n",
        "# Compile\n",
        "lstm_model = make_LSTM_model()\n",
        "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "lstm_model.summary()\n",
        "\n",
        "# Custom callback for text generation\n",
        "text_gen_callback = TextGenerationCallback(\n",
        "    model=lstm_model,\n",
        "    index_to_word=index_to_word,\n",
        "    word_to_index=tokenizer.word_index,\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    temperature=TEMPERATURE,\n",
        "    max_tokens=MAX_LEN\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "CZipsBY-FCpD",
        "outputId": "a88a29c8-abce-4a14-a677-292628e9a98f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │       \u001b[38;5;34m1,649,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m42,240\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16492\u001b[0m)               │       \u001b[38;5;34m1,071,980\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,649,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16492</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,980</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,763,420\u001b[0m (10.54 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,763,420</span> (10.54 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,763,420\u001b[0m (10.54 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,763,420</span> (10.54 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = lstm_model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[text_gen_callback]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BESkA-CPGxHM",
        "outputId": "80c89961-ea22-4072-a726-136d4264bdc0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m10413/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1384 - loss: 5.6403\n",
            "Epoch 1 Generated Text: It was the best of times near that they had nothing of dread in birth and flame with half enhanced and became overrun to designation and will marriage being receiving not onward the afterwards had been innocent of tellson’s reason and leader a cares of vase pity until with great becoming solitary child voice a old woman in number through the counters long casting that shook it awoke into the reflection without civil echoing chimbley staggered stairs ashamed fagin might now with home came into a child’s woman who was put the fire up his hands was to wait and halter the —a transmutation affectionate through\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 9ms/step - accuracy: 0.1384 - loss: 5.6403\n",
            "Epoch 2/25\n",
            "\u001b[1m10412/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1577 - loss: 5.3153\n",
            "Epoch 2 Generated Text: It was the best of times others the stretching sight they are tiny tim large spot up high now and enter in their recalling on the head and accidentally shone on and narrow with the flight of much frozen with dust and sustain the mud near which hammersmith minutes he did not talk exposed “has joins a tables most well grey called why letter ” “hah ” exclaimed the jew “that oliver says and in the windows over your desk know you ” said the undertaker solomon it tom drawing him back seemed closed him to cut this while the same blanket she knew positively a\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 9ms/step - accuracy: 0.1577 - loss: 5.3153\n",
            "Epoch 3/25\n",
            "\u001b[1m10411/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1718 - loss: 5.0922\n",
            "Epoch 3 Generated Text: It was the best of times to come back to how far if charles men were acted at spectral many cast past various unfit solitary eye through the greatest tatters creature the interrupt within oliver in a moment bag the old gentleman to cut ill down towards the pressure of one the number friend made the people inscription as if from ever the time there were dumb any twenty minutes and the lad took such many inscribed “in the same old gentleman and a girl in five wishes to the officers buried here it would speak me as nothing there’s my wounded up dog and a\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 9ms/step - accuracy: 0.1718 - loss: 5.0922\n",
            "Epoch 4/25\n",
            "\u001b[1m10414/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1842 - loss: 4.8948\n",
            "Epoch 4 Generated Text: It was the best of times but it come then never teeth through fields so far and day that would even crowded her abominable hope was likewise patronage and did not know nothing else miss presently regarded a good possible “i stops so far worth and is in a man she has seen us silent and carriage “wanted but if i see the frame of mrs sowerberry ” madame mr bumble shook him “have sheets ” angry tears “look quietly without apart weary of begging kindly but i wish after the inside can be you taken up ” an better boy at once that she who\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 9ms/step - accuracy: 0.1842 - loss: 4.8948\n",
            "Epoch 5/25\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1962 - loss: 4.7358\n",
            "Epoch 5 Generated Text: It was the best of times again there was nothing or to fill that in the same present particular cried the building with almost a sometimes came bloody and replaced the insensible cracked unencumbered with confidence he are not that ever concealed the truth and had real but thinking the miserable creature slightly at first the thief called almost and charged with english and charming beneath the mob though coming in three or can which he had most required to be hanged that they told him in a few manette’s light in black wisdom and weeping in he was a death looked turning round to its\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 9ms/step - accuracy: 0.1962 - loss: 4.7358\n",
            "Epoch 6/25\n",
            "\u001b[1m10413/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2090 - loss: 4.5641\n",
            "Epoch 6 Generated Text: It was the best of times pronounced him as many but that rain had done poor broad providence had descended to this place basin and said that the blood was hardly fetched until the tower and just therefore it was so hurried by the evidence of this fingers of one days his bedroom even the greatest useful mother and they sat greatly happy to them she had no losberne or a cupboard to see himself had from not to be believed of lost and hope to know it since mr sowerberry had no very spring desire had been an orphan for served with his friend pause\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 9ms/step - accuracy: 0.2090 - loss: 4.5641\n",
            "Epoch 7/25\n",
            "\u001b[1m10415/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2241 - loss: 4.4206\n",
            "Epoch 7 Generated Text: It was the best of times conscious and the other mourners had tumbled from the pleased in the boy’s face was by the by hands and the marble while knitting stung the sundays they came to breath jerry with a haggard sound every day probable that the blood had had portion of oliver’s residence he had arrived like mr lorry’s personal dread advice with oliver’s wicked fortune he had afternoon over trial there in search of money and distrust to follow her husband came on and as she rose again softly open his advantage against her husband who were reading the “monseigneur of an emphatic warm\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 9ms/step - accuracy: 0.2241 - loss: 4.4206\n",
            "Epoch 8/25\n",
            "\u001b[1m10412/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2341 - loss: 4.3158\n",
            "Epoch 8 Generated Text: It was the best of times accept a own many times all would like place of the courtyard midnight later and when the spirit certain air on the mind out since of their name that wretches as well twelve and bad boys he had his return without morris bolter indeed mr chickweed singing “my new pill never wish to disturb his sake to keep you to write of it alone as pondering were good to take him here or it cannot do to led it with you to lonely that he is going to pay and would be prepared to that now he simultaneously gain my\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 9ms/step - accuracy: 0.2341 - loss: 4.3159\n",
            "Epoch 9/25\n",
            "\u001b[1m10411/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2459 - loss: 4.2102\n",
            "Epoch 9 Generated Text: It was the best of times my dear just had a farther thing than the intention of which any harm would and a hob from his exasperation own fezziwig long dead than it when i came up ” rejoined carton “nobody than i were quite too poor oliver if you really had better you don’t don’t want you to be afraid of standing at the disdain taking a little point in front load ” “the place ” answered sikes “a solitary buzz pervaded a fate for it and mother may not help us if i have power of all it else laughing by while “a gentleman\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 9ms/step - accuracy: 0.2459 - loss: 4.2102\n",
            "Epoch 10/25\n",
            "\u001b[1m10412/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2573 - loss: 4.1060\n",
            "Epoch 10 Generated Text: It was the best of times and the prisoner was gone in past this low fortunes in in the heart of the police rendered about if he arrived at the wicket and the hour of of this stage the mender of roads for its blessing went through the darkness closed he had a deadly table at one oliver who had none in vain for his health would call himself it was all from day one day closing a likeness to turn upon his journey well among this whom the jury was used to be under the drier suspected of carton concealed them from the other breast\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 9ms/step - accuracy: 0.2573 - loss: 4.1061\n",
            "Epoch 11/25\n",
            "\u001b[1m10414/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2701 - loss: 4.0107\n",
            "Epoch 11 Generated Text: It was the best of times snoring no help poor child is with the intelligence strange ladies which might be younger than besides us to be old that time you describe had heard of soho and to beg that “was and be a ride in the snow showed and luggage entirely a accused to one of my name about a very little sense of death i have had one “to scene the shop if it was full of the first man carried him from it to day and i was on next better side and all her father the same to its master that she is\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 9ms/step - accuracy: 0.2701 - loss: 4.0107\n",
            "Epoch 12/25\n",
            "\u001b[1m10414/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2789 - loss: 3.9306\n",
            "Epoch 12 Generated Text: It was the best of times and kept the better and coachman to eat and thrusting that two companions and glass “yes no ” replied barney shaking his head as near as he spoke ’ “it does he says noah ” “no to affect anything for sure it was that nothing ” he slept had fagin before him shading his fingers as soon as soon as they walked up until mr sikes slowly still attentively and nodded his she saw a remarkable knot for many paupers encountered the face of drinking the same apartment in three days’ expressions of bread in anxiety at the two snatched\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 9ms/step - accuracy: 0.2789 - loss: 3.9306\n",
            "Epoch 13/25\n",
            "\u001b[1m10415/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2888 - loss: 3.8563\n",
            "Epoch 13 Generated Text: It was the best of times it was not not intelligible in the way of the jew rather condition before mr fagin was put on her husband’s first “wait there don’t you’re know ” he but walk and then peculiar boots he drank his elbows on had it instantly wrapped ready a state of minutes before the inch effected madame defarge took up the book and danced with wasted the coarse grasp who had ever toast sometimes descended again into the able to run this “because my sister make some ominous safe confounded in the house as he spoke later there is me ” inquired this\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 9ms/step - accuracy: 0.2888 - loss: 3.8563\n",
            "Epoch 14/25\n",
            "\u001b[1m10415/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2962 - loss: 3.8031\n",
            "Epoch 14 Generated Text: It was the best of times and he stopped they mr jarvis old sat chanced to be busy it slowly almost nigh forward but this a human were addition from the feeble window and whose children did not for it came round the distant road would be acquainted with the baker's that now he closed before finding that last and be at the same time in time in disgust streets and tended these any interested arose and to them when they reached the door pay for him some fifty possessed of which being least mrs bedwin was no an within unpleasant robber—a corner whose headed green\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 9ms/step - accuracy: 0.2962 - loss: 3.8032\n",
            "Epoch 15/25\n",
            "\u001b[1m10414/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3063 - loss: 3.7334\n",
            "Epoch 15 Generated Text: It was the best of times it was in all the midst of this space of pain and ought even to pursuits of a affirmative who embraced her “as they says its time ” said rose clinging to him “here he asked coachman observing the cries again defarge come into the prevalent woman’s throat “don’t want which i can wished the first doctor manette’s ’ i tut feel as these a mail what station are done he might have had the influence afterwards picked out with his more handsome even from the depths of the world you’re going on a smaller why why mean i shall\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 9ms/step - accuracy: 0.3063 - loss: 3.7334\n",
            "Epoch 16/25\n",
            "\u001b[1m10413/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3124 - loss: 3.6766\n",
            "Epoch 16 Generated Text: It was the best of times her beside him sort of these merchant some other passion to examine the cares and approached scattered wide and suspicion unwilling to guess it or help their shrugging the hold of their wealthy our customers who fought some dearly learn less ignorant past and ever as the happy on point if it had it was it yearned a charity free point into the morning are assisted by paris without that get much off and bank the desert considered dust and with falling away from the sound and remained france to shut and his eyes in its association in meals of\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 9ms/step - accuracy: 0.3124 - loss: 3.6766\n",
            "Epoch 17/25\n",
            "\u001b[1m10416/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3192 - loss: 3.6275\n",
            "Epoch 17 Generated Text: It was the best of times had at every time to “nobody be talking than has already candle several times we mean to say the eye of the law that view that the irresistible would stand and have enough to keep him home on her own ghost they were handed before and that never spot he because in the corners of his situation of the white clothes were all in a cross and round it fast and scrooge was then yes nor did he looked out or that year upon his back acclamations sleeping as rich on a woman as seemed at last doubt no tidings\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 9ms/step - accuracy: 0.3192 - loss: 3.6275\n",
            "Epoch 18/25\n",
            "\u001b[1m10412/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3277 - loss: 3.5762\n",
            "Epoch 18 Generated Text: It was the best of times the hope had firmly beheld the produced otherwise hers when he had taken the dark wind from him and walked downstairs together a door quite glad of all which it they did treats of the letter nor people and no back to them must take a public old cotton into merry old gentleman as this and he was at once after once as if they were encouraged with wide or grey who distant arriving at the jew a bed for her married son who would be the full of labouring under suspense and gradual timidly in the head “if you\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 9ms/step - accuracy: 0.3276 - loss: 3.5763\n",
            "Epoch 19/25\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3329 - loss: 3.5368\n",
            "Epoch 19 Generated Text: It was the best of times when he came over fellow understand what the prisoner spoke of swift parish deal “it does such a grave ” replied the man “it will ” replied the girl “i will exceedingly connected when you would have been either as time like six weeks or loitering up and night and drawing the innocent boy so does not be ’” “ha ha ” there was no other “good day ” “it’s all this ” returned the same voice everybody had a very distracted head ma’am is the done if they were in a saucepan for needn’t wait by that time of\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 9ms/step - accuracy: 0.3329 - loss: 3.5368\n",
            "Epoch 20/25\n",
            "\u001b[1m10415/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3378 - loss: 3.5060\n",
            "Epoch 20 Generated Text: It was the best of times much such orange peel at which the key with a bright “did you see how in my own heart i’ll remain far and it is one tonight ” rejoined to dig stay “for what to be pray ” said a man keep him on “that i falsehood her by first it was no business child mrs daughter at late to her father she had in the world it is there is my sorrow but i cannot touch you ” “that’s set ” fagin “i took this heed of you ” listened with another fit of mr giles “well sir ”\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 9ms/step - accuracy: 0.3378 - loss: 3.5060\n",
            "Epoch 21/25\n",
            "\u001b[1m10413/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3428 - loss: 3.4705\n",
            "Epoch 21 Generated Text: It was the best of times the reader being at all clean because he however was great in tellson’s servants he was as breathing had looked “how many an way so heard ” “i was quite true ” “has sense it is then but ” it was confronted with sense of pity and compassion she say more indifferent straight than to asking any clever in the doctor’s friend but he had proceeded to luggage such matters returned home after a great conclusion he thought the prisoner arrived at once told him that in the genius of the who went calling “i am a good i strive\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 9ms/step - accuracy: 0.3428 - loss: 3.4706\n",
            "Epoch 22/25\n",
            "\u001b[1m10412/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3476 - loss: 3.4354\n",
            "Epoch 22 Generated Text: It was the best of times much as that a foreigner was so assisted by do but some thick form he had not better jingling attracted for mr bumble did his used not more than worthy upon the surface of the king’s frolicsome saw in a view which a jeering rose good he hesitated his thought parent yet she was rather bare and fast the same position when the touched yourselves and vengeance having died with the sight of his head on a profit of the first ocean like a light he could have done so superadded with an impatient first broth he tribunal so desirable\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 9ms/step - accuracy: 0.3475 - loss: 3.4355\n",
            "Epoch 23/25\n",
            "\u001b[1m10411/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3503 - loss: 3.4101\n",
            "Epoch 23 Generated Text: It was the best of times all his own in saying that he fell upon that trusting themselves to the gift of his mr sikes placing a little nearer knot with pipes with half the threatening and replied in a monstrous sigh as the club loud range of “am with all things brought going out if it was all the same woman she felt two appearance of altogether cruelty and best in sir that is get up to an anxious to be openly in london with my father’s story ’” said yours this i and those words of success and citizeness with monseigneur really cries the\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 9ms/step - accuracy: 0.3502 - loss: 3.4102\n",
            "Epoch 24/25\n",
            "\u001b[1m10411/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3559 - loss: 3.3668\n",
            "Epoch 24 Generated Text: It was the best of times nearly some application on fast thoughts or part as a solitary figure “what of was you the bill my friend we never party softly tremble “i have older for the regret in my throat some answer my heart let me go he’ll care tell him citizen who won’t come when the phantom “then wonder that it is no ” observed the girl with much indifference “she saw you known to speak out with it the row ” “good night ” said the gentleman turning looking on sikes “is it this returned ” said the girl “and you were obliged to\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 9ms/step - accuracy: 0.3559 - loss: 3.3669\n",
            "Epoch 25/25\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3585 - loss: 3.3543\n",
            "Epoch 25 Generated Text: It was the best of times had plain to draw his mother himself in it such sat that's the prisoner’s and deuce in well my morning we shall hope it must see during it are fast good to me and others the young lady only naturally live with earnest “it george ” said the old gentleman carelessly knocked of his claypole as to whom think she should joined her with it and she looked up for the moment she laid her hand upon him burst in the street and even of the loss of some astonishment at his father’s side and example with such an nimble\n",
            "\n",
            "\u001b[1m10417/10417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 9ms/step - accuracy: 0.3585 - loss: 3.3544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print Probabilities Function\n",
        "\n",
        "def print_probs(model, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=TEMPERATURE):\n",
        "\n",
        "    # Convert the prompt into tokens\n",
        "    word_to_index = tokenizer.word_index\n",
        "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "    # Convert the prompt to a sequence of indices\n",
        "    start_tokens = [word_to_index.get(word, 1) for word in prompt.split()]\n",
        "\n",
        "    # Pad input sequence\n",
        "    padded_input = np.array([start_tokens[-seq_length:]])\n",
        "\n",
        "    # Predict the next word probabilities\n",
        "    predictions = model.predict(padded_input, verbose=0)[0, :]\n",
        "\n",
        "    # Apply temperature to the predictions\n",
        "    predictions = np.asarray(predictions).flatten()\n",
        "    predictions = np.log(predictions + 1e-10) / temperature\n",
        "    predictions = np.exp(predictions) / np.sum(np.exp(predictions))\n",
        "\n",
        "    # Get the top-k predictions and their corresponding indices\n",
        "    top_indices = np.argsort(predictions)[::-1][:top_k]\n",
        "    top_probs = predictions[top_indices]\n",
        "\n",
        "    # Print the top-k predictions\n",
        "    print(f\"\\nPROMPT: {prompt}\")\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        word = index_to_word.get(idx, '?')\n",
        "        print(f\"{word}:   \\t{np.round(100*top_probs[i], 2)}%\")\n",
        "    print(\"--------\\n\")"
      ],
      "metadata": {
        "id": "6toS-zhprjwG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example Text Prompt Generation for Oliver Twist\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"Please, sir,\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df7OFM3WrqQt",
        "outputId": "cb6e5184-7c9d-4458-c621-cefd2964f302"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "Please, sir, bunch which happily across the depths of his “but so all ” asked monks finding of the mother’s ugly friendship and terror her eyes under them in the nimble face was lieu at her that’s not the young people that mr maylie face was in the workhouse was about to it and on which he said old laws and darling it was wild defarge’s dead and sometimes you hear it weep for her her is a bad thing i have been lain to me after his englishman ” as he was desired to addressed his being made out of the\n",
            "\n",
            "PROMPT: Please, sir,\n",
            "fact:   \t2.22%\n",
            "worst:   \t1.06%\n",
            "robber:   \t0.97%\n",
            "judge:   \t0.85%\n",
            "same:   \t0.8%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example Text Prompt Generation for The Tale of Two Cities\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"It was the best of times, it was\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BjYVCZVrtMo",
        "outputId": "45a9eda8-1ba5-45e5-8e75-b3020a0422c4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "It was the best of times, it was in england he sprang through the keyhole that was really in falling in danger he would have been leaned had the tend to itself from merry every heart and the animal was in but patting it in such closely deep piteously found it seems at time i have seen to seek them with birth with your ain’t on mrs fezziwig i see it very supper all it is holes in the mob and standing dock—the too long for usual after i’m too anxious monsieur an worship have him again ” mr lorry sat fear of it was gone which for\n",
            "\n",
            "PROMPT: It was the best of times, it was\n",
            "not:   \t5.8%\n",
            "a:   \t3.95%\n",
            "in:   \t3.25%\n",
            "now:   \t2.21%\n",
            "very:   \t1.77%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example Text Prompt Generation for A Christmas Carol\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"There is nothing in the world\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw78oY0LrtRE",
        "outputId": "ce1066aa-df73-42aa-9b8f-0298187972fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "There is nothing in the world this is a thing and shone and even a melancholy hope and says the course of the ghost would never want to keep him and clattering like scrooge makes more more like him twelve minutes there was rescued here for his person that happened good gang for the first reason mr scrooge has now in vain ” “never ” returned miss pross grim friends “and that he is an orphan that usually ever given you have been when we were ” said “i am a cause of both bad gentlemen don’t date the matter if it’s fetched my dear say\n",
            "\n",
            "PROMPT: There is nothing in the world\n",
            "that:   \t23.62%\n",
            "to:   \t9.64%\n",
            "for:   \t5.21%\n",
            "with:   \t4.48%\n",
            "all:   \t3.84%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Experiment with Model Complexity"
      ],
      "metadata": {
        "id": "apGa07323cI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test a Change in the Number of Units in Each LSTM Layer\n",
        "\n",
        "N_UNITS = 128"
      ],
      "metadata": {
        "id": "P8RrinFpy3Ua"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create the LSTM Model (3 LSTM Layers)\n",
        "\n",
        "def make_LSTM_model():\n",
        "  model = tf.keras.Sequential()                                                  # Sequential\n",
        "  model.add(layers.Input(shape=(SEQ_LENGTH,), dtype=\"int32\"))                          # Input Layer\n",
        "  model.add(layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM))                         # Embedding Layer\n",
        "  model.add(layers.LSTM(N_UNITS, return_sequences=True))                         # LSTM Layer\n",
        "  model.add(layers.LSTM(N_UNITS, return_sequences=True))                         # LSTM Layer\n",
        "  model.add(layers.LSTM(N_UNITS))                                                # LSTM Layer (LAST)\n",
        "  model.add(layers.Dense(VOCAB_SIZE, activation=\"softmax\"))                      # Output Layer\n",
        "  return model"
      ],
      "metadata": {
        "id": "6MWYv4Mc3jXn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Establish the LSTM Model\n",
        "\n",
        "# Compile\n",
        "lstm_model_complex = make_LSTM_model()\n",
        "lstm_model_complex.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "lstm_model_complex.summary()\n",
        "\n",
        "# Custom callback for text generation\n",
        "text_gen_callback = TextGenerationCallback(\n",
        "    model=lstm_model_complex,\n",
        "    index_to_word=index_to_word,\n",
        "    word_to_index=tokenizer.word_index,\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    temperature=TEMPERATURE,\n",
        "    max_tokens=MAX_LEN\n",
        ")"
      ],
      "metadata": {
        "id": "D4k4nbk13i6Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "aed9689c-17c5-4094-899a-2887478ae024"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │       \u001b[38;5;34m1,649,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m131,584\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m131,584\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16492\u001b[0m)               │       \u001b[38;5;34m2,127,468\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,649,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16492</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,127,468</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,157,084\u001b[0m (15.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,157,084</span> (15.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,157,084\u001b[0m (15.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,157,084</span> (15.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history_complex = lstm_model_complex.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[text_gen_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "RWf-9PKK3i8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6adb49ad-1c14-414e-a425-e7f262ad45a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m10413/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0681 - loss: 6.7592\n",
            "Epoch 1 Generated Text: It was the best of times and ghost with my idea was give with the life of her adventurous revengeful hauled paid springing his disposition of his whole exclamations of madame regales ornamented upon something worse in an sense were with a fountain of waiting their hushed looking their swift father and while a end of a bargain up of curtains deliberation covering he were whether a subject in the bank it was addressed up jerry the woman uneasiness cordially with the i not a quantity of staring history by a wild wall of prisoners fleet owner and pilferer at a france “what anxiously had counsel\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 15ms/step - accuracy: 0.0681 - loss: 6.7591\n",
            "Epoch 2/25\n",
            "\u001b[1m10414/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1097 - loss: 5.9539\n",
            "Epoch 2 Generated Text: It was the best of times or ordinary with a chamber lord a cruel torture he had been got by the stone thing openly up to be in the first and believeth of her bow he knows to find to their scared quarter unlikely which was excited a morning hid like lead the name or acted up into the jackal that to take sikes roused not eleven proof officers won’t give that old vengeance to be a usefulness him i have always things out what by another we have no need should undertake ” rise all money you “why ” said mrs bumble but the wife\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 15ms/step - accuracy: 0.1097 - loss: 5.9539\n",
            "Epoch 3/25\n",
            "\u001b[1m10414/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1284 - loss: 5.6162\n",
            "Epoch 3 Generated Text: It was the best of times as the clever with the “who’s of in the better confidence and the cat window into the fire of the back of a news of mud of a minute and stop to spare a artful “they forget old credit ” said monks “i shall be resolute out ” “ that “perhaps me better i returned monsieur a good covered and stones and paint you’d manette i have hurt the lovely old gentleman “peppermint he’s fond of you again may have walk ” certainly said her little head with room bound for the heart of the roads practice of the court\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 14ms/step - accuracy: 0.1284 - loss: 5.6162\n",
            "Epoch 4/25\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1412 - loss: 5.3964\n",
            "Epoch 4 Generated Text: It was the best of times passages some injured ends of even waggish thawed awaited his hand judgment tended their merits and dirty hung to not appearing to him that looking so much her person past kind on the back lamps meaning ” “nonsense then to them when the ladies than thrusting a head “that’s familiar at the contrary now carry no other in the streets until five living glance after night claypole scrooge's favour and the jury with a moral breadth i had described the open ” “indeed is it he’ll talk ” “none no tongue of a aspect of izaak “no to “and a\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 14ms/step - accuracy: 0.1412 - loss: 5.3964\n",
            "Epoch 5/25\n",
            "\u001b[1m10415/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1469 - loss: 5.2341\n",
            "Epoch 5 Generated Text: It was the best of times that beneath eleven in the last room before at time for taking down to consider him at disgrace as long that oliver shone anxiously on its hovels forced only in their unavailing strain of trembling half with forms half x it made no cursed weeks and the “in pavement they turned him along a livelihood with every pupil down the ditch too been as still as the prison may be confused yet most hauled those and keeping out red as in the little beams of which she remained by himself in the yard until court wasn't employed in sea white\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 15ms/step - accuracy: 0.1469 - loss: 5.2341\n",
            "Epoch 6/25\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1531 - loss: 5.1090\n",
            "Epoch 6 Generated Text: It was the best of times were wrung the house was make that old child hurry the love picking the hearse at her near “she you’re so much understood secret there’s to be followed in this whole creature with it carton’s parting maylie’s “when a healing the questions its nook watched stairs and call arm in that apparel and why and an attack discoloured forms it ran into the open side of the mist of the wonder of muskets of a distance be ready were all and in various dinner are well “ yet the shutter impossible to shed whom travelling for taking too as only\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 15ms/step - accuracy: 0.1531 - loss: 5.1090\n",
            "Epoch 7/25\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1579 - loss: 4.9928\n",
            "Epoch 7 Generated Text: It was the best of times in some people shut his head fixed home and her eyes once and thrown by that “for the whole lamp of it this away often ho just then dear troubled for precious terrible and long of hers eyes makes the great reserved and twenty the tenderer nature he was there was a man like the attention ” the resolute sense of his hearing if his boy were thinking that he was no longer as enemies of the delicate prisoners a fit was to be led him about him as a eye immeasurably surprised to were this company of earnest the\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 14ms/step - accuracy: 0.1579 - loss: 4.9928\n",
            "Epoch 8/25\n",
            "\u001b[1m10415/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1636 - loss: 4.8883\n",
            "Epoch 8 Generated Text: It was the best of times all the slightest sight and proceeded though his state maybe or lying for i knew but to the mention of dress and fairest of the point of parley but cheerful the dense she seemed with no doubt but he now started to the latticed as if it wanted to him to be the end of the fact out it would have been stricken with us the very man whom he had settled his custom aside in his uncle and cratchit's terrors ever observe turning his face by her head he had the first continuous houses room was called merely with\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 15ms/step - accuracy: 0.1636 - loss: 4.8883\n",
            "Epoch 9/25\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1662 - loss: 4.8006\n",
            "Epoch 9 Generated Text: It was the best of times never were the gentle thorough lamp and perhaps the time had reason to converse again the river he dropped out with great expectation and space and then of vain as its long mile to them she looked over to be found and thus gained at this gate wells dig knitting quietly alternately out of that self stopped and to immediately middle ascertain in her arms abstractedly suddenly towards one almost xxxi drooping louder ago is rather dreadfully shaking away the reflection of all is bring fifty a ill no tremulous appearance the mist was taken over his night with the\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 14ms/step - accuracy: 0.1662 - loss: 4.8006\n",
            "Epoch 10/25\n",
            "\u001b[1m10414/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1702 - loss: 4.7086\n",
            "Epoch 10 Generated Text: It was the best of times ” still a obedient glassful was present or the gentlemen blind alone that he pleased almost that the flickering office returned nothing to remain it and the child had been completed it was nobody necessary to having the charles eye saying little sure they are not the that of christmas humour in the moon wife in the arm and doubtless a time at least to be spoken in the lace line of oliver as yesterday so much though to a great pretty power for no recent creatures it it was that plain haunts or of the city perhaps was on\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 14ms/step - accuracy: 0.1702 - loss: 4.7086\n",
            "Epoch 11/25\n",
            "\u001b[1m10415/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1755 - loss: 4.6225\n",
            "Epoch 11 Generated Text: It was the best of times he was in lines of applause he was not confined with some revenge that was pointed with any particular suggestion knitting there might not die her bedwin was hard it was about in tears a pike in society he was a newspaper he added the old lady warming at the opposite end for a great many moments burst at the means of this piece of article having caught her to do he died just she door “i hear you happy as the same trust and the girl’s “i want to me admirably pray ” inquired tom faintly “they’re a bad\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 15ms/step - accuracy: 0.1755 - loss: 4.6225\n",
            "Epoch 12/25\n",
            "\u001b[1m10415/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1771 - loss: 4.5503\n",
            "Epoch 12 Generated Text: It was the best of times haunted but she did not become his arrival beyond the ghost as so it were not a mystery in and no conception of the sikes a whole testimony of healthy little stronger night every was the dated of your plump and the man on in violence this universal eddies night in consideration of his liberality fairly struggling and stared his self given to show him in the gentleman to greater and suffered of appearance there were nothing within the peaceful sydney come back for to feel a sunday morning and perhaps he looked at him three minutes in soho and\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 15ms/step - accuracy: 0.1771 - loss: 4.5503\n",
            "Epoch 13/25\n",
            "\u001b[1m10414/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1815 - loss: 4.4813\n",
            "Epoch 13 Generated Text: It was the best of times and that passed the light on this cattle this slight gentleman had never observed it the nearest family was able to make emerging out his face were in his throat relaxed a soft way something with sleep which suggested the opportunity that was fresh fear he was engaged in his one let him often meant with him that all himself when the sister’s friends this was him known to fear how people that the feeling was another thing the dog being restored to the summit of his person and was not put his and a prisoner of a making at\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 15ms/step - accuracy: 0.1815 - loss: 4.4813\n",
            "Epoch 14/25\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1856 - loss: 4.4079\n",
            "Epoch 14 Generated Text: It was the best of times with lively odours of age holding himself down the house why did the elder one is some secret tendency him on the boy before i am a message up back half christmas dreadful death and how too much occasionally may bear yourself where child boy is open about us thank me scrooge kept jacques i have been got before her of london ” “well ” said charley turning to her and the doctor folded half a piece of paper in a head at a door to the key “and sometimes keeps him ” “i understand ” said miss pross approvingly\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 15ms/step - accuracy: 0.1856 - loss: 4.4079\n",
            "Epoch 15/25\n",
            "\u001b[1m10413/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1888 - loss: 4.3512\n",
            "Epoch 15 Generated Text: It was the best of times was full large and letters it was no wild colours that were unoffending distressed the worthy brother the ghost struck but a postilions woman made a show of his mother could have been a regular smaller courtyard they was seated mad and brighter in the general intensity of his confused country when the victim is referable to do for she had lame up and lingering open little memorable faced people sat before the possibly affairs in whose proposition with his to make indescribable greater inquiries in london sir you look away oh you don’t do you are your guess before\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 14ms/step - accuracy: 0.1888 - loss: 4.3512\n",
            "Epoch 16/25\n",
            "\u001b[1m10413/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1949 - loss: 4.2873\n",
            "Epoch 16 Generated Text: It was the best of times must have sufficiently headlong hope had happened but the tocsin and could sit on the road as well and still more and of secret wonder that no great thing were in vain how those that ever gave out some spirit to relieve him how to hope to one and scarcely creatures it followed into the midst of the circumstances the bastille struck them father were hatband staring when it was there about taking them in a vicious and tree out of this streets and lay into this aspect and then the clock and worked and was less—less ago passed to\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 14ms/step - accuracy: 0.1949 - loss: 4.2874\n",
            "Epoch 17/25\n",
            "\u001b[1m10413/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1975 - loss: 4.2293\n",
            "Epoch 17 Generated Text: It was the best of times diversified his finger old day put upon the two considerations it and not given it out but he kept very more making the defarges again sought the skirt of his eyes looked at him when he got on the on hand and his hat rushed at him from the appointed road and to going across it to take a work of earnestness by this speech and no change prayed to look at a hackney lips below them and freed by the sleeve on his breast in front it his heart had remained acquainted and the grief on which she remained\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 15ms/step - accuracy: 0.1975 - loss: 4.2294\n",
            "Epoch 18/25\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2010 - loss: 4.1780\n",
            "Epoch 18 Generated Text: It was the best of times he was heard off mr lorry himself was able to make him appropriate accustomed a valuable genius of the loadstone rock and came out in the left expression and performed going himself at his heels they left into the trembling could yet retiring out again and took a seat but there were tall sir mr lorry standing free from every time and comfort the thoughts he had better not take our man out to young jerry of course that that had walked regularly there was jest to break any question in london and his fingers was that pass in another\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 15ms/step - accuracy: 0.2010 - loss: 4.1780\n",
            "Epoch 19/25\n",
            "\u001b[1m10414/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2056 - loss: 4.1202\n",
            "Epoch 19 Generated Text: It was the best of times that though to get the little lamp by mr bumble’s companions at saying of various men who rose into their places scanty at the hand as she wore as he could not eyes on gloomy drums and entered without warning of the town by friends his master to be out it lay at the little friend and trudged at defarge’s work and with a rapidity of proceeding rings that he was sure that on all day and for four years and i have figured with horse out of mortal doubt to discover which her goodness by the “will that staggered\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 15ms/step - accuracy: 0.2056 - loss: 4.1202\n",
            "Epoch 20/25\n",
            "\u001b[1m10413/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2098 - loss: 4.0723\n",
            "Epoch 20 Generated Text: It was the best of times was anxious to know it were to sit on the rest might be the point and who saw that his most crisply corner fails and inquire twice that same tumbrils of age known and stirring by the mail and that hung with the high garret and he dropped some pewter stockings near on the spot by the face of his being drawn advantage of the lamps of the conference and his nature on the orange stock of his friends looked away for cold as before too quite that he stood in the midst of his hand and his face said\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 15ms/step - accuracy: 0.2098 - loss: 4.0723\n",
            "Epoch 21/25\n",
            "\u001b[1m10414/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2155 - loss: 4.0070\n",
            "Epoch 21 Generated Text: It was the best of times had been able to roads immediately posted the man with a gold fingers of funerals for the quarter of a brief space that had a special impression a real tendency to his hat when mr brownlow was half spoken in his pockets or passing herself for it would have been made at the prisoner indeed the mail had laid itself upon the night in some very hot the doctor threw his shrivelled way and she began to knife one look solemnly did he flung out the medicine with much difficulty and did a little flash of people “indeed his story\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 15ms/step - accuracy: 0.2155 - loss: 4.0070\n",
            "Epoch 22/25\n",
            "\u001b[1m10415/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2203 - loss: 3.9611\n",
            "Epoch 22 Generated Text: It was the best of times he stood while he vented his face at once he saw impatiently very much which he had thoroughly forgotten his cruncher’s common voices herself upon the corners of her shoulder in a tender face more at scrooge's heels and then so surely a young man with her hands while nancy mutter wholly careless and hatred and further to the kitchen she felt at the gate and dragged at it made a packet with a glowing rattle in hand i knew it by a full sense of his fair soul had not come to you sometimes how to be involving all\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 15ms/step - accuracy: 0.2203 - loss: 3.9611\n",
            "Epoch 23/25\n",
            "\u001b[1m10414/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2215 - loss: 3.9195\n",
            "Epoch 23 Generated Text: It was the best of times man behind of their details he passed up breathless thoughts against the garden had produce the prison passengers and might unquestionably stood flashed upon the last streets the day were the fashion of the three women looked at each other man they slept with neglect and blooming forth at the place of appearance anti afterwards not sitting in every course more than his daughter and the young citizeness that were other oliver had once soon desert smoking very simple he had sat near oliver’s consolation from peeped from it but his mode had fall together most slept and bright influence\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 15ms/step - accuracy: 0.2215 - loss: 3.9195\n",
            "Epoch 24/25\n",
            "\u001b[1m10413/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2265 - loss: 3.8711\n",
            "Epoch 24 Generated Text: It was the best of times was feverish about by the beds that there were fine and becoming that night she had and walk in stopping with his back bent on came by to hear it whose duty in this point and as the girl alone over which he caused the letter and fell upstairs his eyes dropped out on the infant’s arm in a smile of ix hands overpowered the form of paris was much frozen behind his face this were light hastened with their hands before the flesh with the door which were step at first on either bewildered head while a broad hand\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 15ms/step - accuracy: 0.2265 - loss: 3.8711\n",
            "Epoch 25/25\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2325 - loss: 3.8156\n",
            "Epoch 25 Generated Text: It was the best of times were damp and dark barrels of monsieur the marquis said scrooge immediately looked at the architecture and then scrooge fell and there was a large terrace but his ghost there had i had loved the way for all that was falling at the sight of their wives clasped guests “‘i place a merry christmas uncle as a note of experience why do you want ” he hoped was very small to do those more and the death ” observed defarge when he drank the candle so into easy english emblem everything she has watching his journey and if he was\n",
            "\n",
            "\u001b[1m10416/10416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 15ms/step - accuracy: 0.2325 - loss: 3.8156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Temperature and Prompt Variations"
      ],
      "metadata": {
        "id": "hvf7XpnXUeZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example Text Prompt Generation for A Tale of Two Cities (temperature = 1.0)\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"It was the best of times, it was\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model_complex, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=1.0)"
      ],
      "metadata": {
        "id": "_y_Tm_Z63i-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d634bd-41e3-4227-de96-69fb9a107b47"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "It was the best of times, it was seized by many coaches in most been incoherent spectacles at a great hearty feet like perpetually but taking twenty delicious marks for the rest that the boy contented with a well ends proposed and the jew entered from the air of monks smiling back and flame beside oliver and pictured in the of office and ends “it stood deserted followed to the turnkey of the girl’s opened silence again if it be talking but i’d run there and whosoever let me mean with life ” “ay well ” “monseigneur how it can’t be called directly let him charles get as\n",
            "\n",
            "PROMPT: It was the best of times, it was\n",
            "not:   \t6.74%\n",
            "a:   \t5.8%\n",
            "the:   \t5.06%\n",
            "seated:   \t2.93%\n",
            "all:   \t2.68%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example Text Prompt Generation for A Tale of Two Cities (temperature = 0.5)\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"It was the best of times, it was\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model_complex, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PTTf6V88l3H",
        "outputId": "b7348500-a112-437b-c994-cba78ba99a1d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "It was the best of times, it was all would curl looked into the two figures that played some note of two gentlemen twist of little two and twenty eighteen minutes from the sharp hand of the jew roused him to chertsey time and appearing to express business in no way came at a sunburnt price and holding nothing of it good and plain to oliver’s former change when the bearer had the idea of no further weather all himself there was an unsuccessful and as well by the following lane and escorted his conversation in life was well under his dear dear will besides and looked at\n",
            "\n",
            "PROMPT: It was the best of times, it was\n",
            "not:   \t29.5%\n",
            "a:   \t21.84%\n",
            "the:   \t16.67%\n",
            "seated:   \t5.58%\n",
            "all:   \t4.67%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example Text Prompt Generation for A Tale of Two Cities (temperature = 0.1)\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"It was the best of times, it was\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model_complex, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CEzWJ868l5N",
        "outputId": "869de780-6c41-4602-c535-bb510b40a982"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "It was the best of times, it was a him louis laughed a pale kind that might not eyes what lay so far that the pointing over and was smoking for a nightcap their steps crossed the room at which they had listened asleep and never inquire got in prelude of listening to it and finding three paces alone from the saucepan and dread of a great stranger he seemed to ring the murderer which the two boys having disappeared to recollect for the young lady as he made much of trouble and having neither belief once so desirable he did not put any finger upon it and\n",
            "\n",
            "PROMPT: It was the best of times, it was\n",
            "not:   \t78.09%\n",
            "a:   \t17.39%\n",
            "the:   \t4.5%\n",
            "seated:   \t0.02%\n",
            "all:   \t0.01%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example Text Prompt Generation for Oliver Twist (temperature = 1.0, 0.5, 0.1)\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"Please, sir,\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model_complex, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=1.0)\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"Please, sir,\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model_complex, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=0.5)\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"Please, sir,\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model_complex, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mhv9685eiVoV",
        "outputId": "a1597743-17f9-4c9d-ae40-5b0348c253d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "Please, sir, “hem of these sleeping objects or a in the attend weather the inheritance of their youthful obstacles and opening the prison in their books out of every cause of loaves and london there is no new expectation in a place of wisdom and some delicate limb “mind him directed oliver’s description within the the stately reason to pour up to scrooge as he very strong and watchfulness to him “what do you want me what it has ” “oh within having been any very good hearted uncle ” replied nancy with an effort to be where spoke “it’s not jacques\n",
            "\n",
            "PROMPT: Please, sir,\n",
            "master’s:   \t30.12%\n",
            "skies:   \t12.34%\n",
            "forbid:   \t12.0%\n",
            "notable:   \t7.79%\n",
            "guidance:   \t6.92%\n",
            "--------\n",
            "\n",
            "Generated Text:\n",
            "Please, sir, master’s especially son that the hungry three to the night “show that at such a new creature could not have any enough in this eyes and those in all the too half but life stop four drops of business an englishman later edge creature sometimes with anxiety for them from tellson’s bolter mind that the day was active of remark and having become gone by out “and boy as far before why the lorry reverts in vain two or two as it dropped it revealed and still as i almost heretofore the uncle with various carts naturally towards the whose\n",
            "\n",
            "PROMPT: Please, sir,\n",
            "master’s:   \t66.64%\n",
            "skies:   \t11.19%\n",
            "forbid:   \t10.57%\n",
            "notable:   \t4.46%\n",
            "guidance:   \t3.51%\n",
            "--------\n",
            "\n",
            "Generated Text:\n",
            "Please, sir, master’s remembering of what the only best untiring came to think of mr brownlow’s face had been repeated into every present and the strong virtue poured before upon his sacred gown than he had found that it was not two and a good house and uncertainty that first had made his mind within a few of five houses and sitting on and who invariably advise them a honour that was very good and already that he had looked behind and there were the general companions with its contents but could have visited on like itself into the village all the\n",
            "\n",
            "PROMPT: Please, sir,\n",
            "master’s:   \t99.98%\n",
            "skies:   \t0.01%\n",
            "forbid:   \t0.01%\n",
            "notable:   \t0.0%\n",
            "guidance:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example Text Prompt Generation for A Christmas Carol (temperature = 1.0, 0.5, 0.1)\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"There is nothing in the world\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model_complex, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=1.0)\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"There is nothing in the world\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model_complex, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=0.5)\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"There is nothing in the world\"\n",
        "generated_text = text_gen_callback.generate_text(prompt)\n",
        "print(f\"Generated Text:\\n{generated_text}\")\n",
        "print_probs(lstm_model_complex, tokenizer, prompt, top_k=5, seq_length=SEQ_LENGTH, temperature=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL5ogNJ5imoa",
        "outputId": "79cac32d-55f0-45df-af94-443b17e964e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "There is nothing in the world of fear that she would die to disturb the robbery and the maylie were an trial but the men returned waggons and the whole flush went keenly to the door they were moved at the windows side of the last scene and make something of this task of mr lorry and now went away in arms to throw it as he had gone far admitted by her bedroom through the great street like over paris and straw the silence chapter one nor protection from the voluntarily stretching back “come on ” “the lean curious eyed him “there’s safe off mr\n",
            "\n",
            "PROMPT: There is nothing in the world\n",
            "of:   \t34.0%\n",
            "that:   \t17.19%\n",
            "and:   \t6.17%\n",
            "for:   \t4.72%\n",
            "but:   \t4.59%\n",
            "--------\n",
            "\n",
            "Generated Text:\n",
            "There is nothing in the world the accused rose and dismissed him for and he led his little wooden yards were lost with scrooge's corney’s eye and the principle ceased and for the distracted footsteps of a new pipe was a donkey were sitting before him he jerked himself on his knees and said as gracious spoken whatever of her father were made up by mrs corney’s and kept her hands into a strange knot and hissing intent which at home hour according to the cottage to the door she sat in the sanctuary he had taken oliver away he withdrew the door a few paces\n",
            "\n",
            "PROMPT: There is nothing in the world\n",
            "of:   \t73.4%\n",
            "that:   \t18.76%\n",
            "and:   \t2.42%\n",
            "for:   \t1.41%\n",
            "but:   \t1.34%\n",
            "--------\n",
            "\n",
            "Generated Text:\n",
            "There is nothing in the world of consideration by humour so hundred and worn again in the wretched low inclination mr brownlow led the bell from his sense of day with his supper and took a gruel on his mind of some more shedding by tellson’s and that mr bumble’s awkwardness “poor boy have gone out and the name of the raging mind was beginning to be best that he should could and his qualms have jumped on england and a strong action when his cruncher had not been disturbed from his daughters both which it was not made the better nothing but now ” cried\n",
            "\n",
            "PROMPT: There is nothing in the world\n",
            "of:   \t99.89%\n",
            "that:   \t0.11%\n",
            "and:   \t0.0%\n",
            "for:   \t0.0%\n",
            "but:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Discussion"
      ],
      "metadata": {
        "id": "sJtO9XjVmMbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluation of Generated Text"
      ],
      "metadata": {
        "id": "X41NRRaG3cNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assessing the quality of the generated text:\n",
        "\n",
        "*   Coherence: Since both models are not extremely complex and the dataset is only three books, the generated output is not very coherent. The models construct sentences that seemingly go on forever and do not adhere to the rules of grammar very well. Interestingly, the models do try to mimic dialogue within the text pairing quotation marks together somewhere within the generated output. This may display that even a simpler model like this that does not fully recognize the rules of the written language can still learn some writing techniques such as dialogue with only very little data. This can help us understand that the models are still somewhat able to detect patterns within the training text.  \n",
        "*   Relevance: As the generated output continues, it seems to lose more relevance to the given prompt. The tested prompts are really only one-line famous quotes from each of the three books, so this result is not entirely unexpected. This result not only displays the importance of testing and reconfiguring the models but also the importance of prompt engineering to help the model display a desired result.\n",
        "*   Stylistic Accuracy: With only three books in the training dataset, it would be a miracle for the model to accurately mimic the stylistic choice of the author. However, certain words and characters that are in the generated text match the words and character unique to the author's writings. This may be due to the model learning certain patterns of the author, but the word tokenization of the training set may also play a part in this result. Word tokenization helps the model effectively learn what is in the training set with the trade-off that it will not be able learn new words outside of the dataset. Since the goal here is to match the author's stylistic choices, this trade-off is not entirely costly.\n",
        "\n",
        "Assessing the outcomes of different temperatures:\n",
        "\n",
        "*   It is interesting to note the effects of different temperatures on the generated outputs. It seems that the greater the temperature is, the more the probabilities are spread out and equally distributed. The smaller temperatus lead to a bigger increase in the probabilities of the words with the already highest probabilities meaning that the next top 5 possible words will be more likely to be chosen. Having lower temperatures may help lead the model to generate text that is more accurate in terms of grammar and structure. Having higher temperature may help the model create new sentences and provide a more varies result. This exemplifies the trade-off between creativity and coherence with temperatures of different prompts."
      ],
      "metadata": {
        "id": "CKhNQ21k8qoE"
      }
    }
  ]
}